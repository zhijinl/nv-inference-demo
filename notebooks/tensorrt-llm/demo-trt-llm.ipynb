{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cca6fd12-e58e-42b0-8fb4-571d4a99e5f7",
   "metadata": {},
   "source": [
    "# Getting started with TensorRT-LLM and Triton Inference Server\n",
    "\n",
    "This hands-on tutorial is based on the TensorRT-LLM demo from ai-PULSE by Scaleway, which can be found here: https://github.com/scaleway/ai-pulse-nvidia-trt-llm/tree/main\n",
    "\n",
    "In this tutorial, we will cover\n",
    "- How to convert llama 2 models to TensorRT-LLM format\n",
    "- Set-up Triton Inference Server with llama 2 models optimized using TensorRT-LLM\n",
    "- Benchmark the inference performance of Triton + TensorRT-LLM pipeline vs vanilla Python HuggingFace pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0600ce6-4c77-48b3-abda-d5494bbd6ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun 18 11:52:30 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:DA:00.0 Off |                    0 |\n",
      "| N/A   37C    P0              63W / 400W |      2MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ca9d08-2308-46cd-8c02-f3298ba6c394",
   "metadata": {},
   "source": [
    "## 1. Setup the environment\n",
    "\n",
    "First let's clone the TensorRT-LLM github repo and be sure to use the correct version for this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cffe130-5ed3-408c-8c1f-932d7009b514",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'TensorRT-LLM' already exists and is not an empty directory.\n",
      "fatal: detected dubious ownership in repository at '/workspace/nv-inference-demo/notebooks/tensorrt-llm/TensorRT-LLM'\n",
      "To add an exception for this directory, call:\n",
      "\n",
      "\tgit config --global --add safe.directory /workspace/nv-inference-demo/notebooks/tensorrt-llm/TensorRT-LLM\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/NVIDIA/TensorRT-LLM.git \n",
    "!git config --global --add safe.directory /workspace/notebooks/tensorrt-llm/TensorRT-LLM\n",
    "!cd TensorRT-LLM && git checkout v0.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006ed03b-5559-4bd6-8be6-80bb505c2875",
   "metadata": {},
   "source": [
    "Next, let's download the llama 2 models, if it is not already done yet. \n",
    "\n",
    "For this you need to go to the models [website](https://llama.meta.com/llama-downloads), register, then an email with a custom URL will be sent to you allowing you to download the llama models.\n",
    "\n",
    "To proceed with the download, first clone the llama repo, then launch the download script. When prompt with URL, just enter the URL that you received via email before. For this tutorial, we will need to download 1 model: the 7B-chat. Put the downloaded model inside `./llama-models` folder.\n",
    "\n",
    "Note: **the download could take some time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20a27941-4b55-470d-a059-2220f87c3ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'llama' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/facebookresearch/llama.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee4f01c-71bb-4226-9a26-0303df954ac5",
   "metadata": {},
   "source": [
    "**Note: run commands in the following cell in a separate terminal (without the prepending !)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63c73792-c0a4-4213-9fda-485fe7765b8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the URL from email: ^C\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p llama-models\n",
    "!cd llama-models && ../llama/download.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c957cde8-88b8-4de2-b668-98e93b1b7441",
   "metadata": {},
   "source": [
    "We also need to clone the huggingface transformers repo, to be able to use the conversion script to convert llama 2 models checkpoint format to huggingface's Transformers format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3310dae6-0996-46be-ac2a-ce539ae67597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 524K\n",
      "drwxr-xr-x  3 99 99 4.0K Mar  4 15:51 \u001b[0m\u001b[01;34m.\u001b[0m\n",
      "drwxr-xr-x 15 99 99 4.0K Jun 17 08:52 \u001b[01;34m..\u001b[0m\n",
      "-rw-r--r--  1 99 99 6.9K Jul 15  2023 LICENSE\n",
      "-rw-r--r--  1 99 99 4.7K Jul 15  2023 USE_POLICY.md\n",
      "drwxr-xr-x  2 99 99 4.0K Feb 27 23:12 \u001b[01;34mllama-2-7b-chat\u001b[0m\n",
      "-rw-r--r--  1 99 99 489K Jul 13  2023 tokenizer.model\n",
      "-rw-r--r--  1 99 99   50 Jul 13  2023 tokenizer_checklist.chk\n"
     ]
    }
   ],
   "source": [
    "!ls -lah --color llama-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1988b286-4c55-4587-85fe-b9b031a8bac3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'transformers' already exists and is not an empty directory.\n",
      "fatal: detected dubious ownership in repository at '/workspace/nv-inference-demo/notebooks/tensorrt-llm/transformers'\n",
      "To add an exception for this directory, call:\n",
      "\n",
      "\tgit config --global --add safe.directory /workspace/nv-inference-demo/notebooks/tensorrt-llm/transformers\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/transformers\n",
    "!cd transformers && git checkout v4.39.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ad2073-8853-4bb2-899e-1b5b25feb39e",
   "metadata": {},
   "source": [
    "Now convert meta checkpoint weights to huggingface format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f43ba8e-16c7-49bd-9b38-95688784f838",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "Fetching all parameters from the checkpoint at ./llama-models/llama-2-7b-chat.\n",
      "Loading the checkpoint in a Llama model.\n",
      "Loading checkpoint shards: 100%|████████████████| 33/33 [00:06<00:00,  4.95it/s]\n",
      "Saving in the Transformers format.\n"
     ]
    }
   ],
   "source": [
    "!cp llama-models/tokenizer.model llama-models/llama-2-7b-chat/.\n",
    "!mkdir -p hf-weights\n",
    "!python ./transformers/src/transformers/models/llama/convert_llama_weights_to_hf.py --input_dir ./llama-models/llama-2-7b-chat --model_size 7B --output_dir ./hf-weights/7B-chat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34bc658-9d92-43b1-b875-6dbac4bfba33",
   "metadata": {},
   "source": [
    "## 2. Compile llama 2 models to TensorRT-LLM engines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217f10e5-e782-4369-b8fa-28ba80057e14",
   "metadata": {},
   "source": [
    "Just like TensorRT, TensorRT-LLM provides APIs to compile / convert build LLMs to TensorRT engines. In this example, the conversion steps were implemented already in the `TensorRT-LLM/examples/llama/build.py` script provided by TensorRT-LLM repo. We can analyze the script to see how TensorRT-LLM APIs were used to build the LLM model and load the trained weights.\n",
    "\n",
    "The TensorRT-LLM team is working on high-level APIs to make the conversion steps easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7476bae2-1004-4ced-8f53-3ccdee3bffc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/18/2024-11:50:22] [TRT-LLM] [I] Serially build TensorRT engines.\n",
      "[06/18/2024-11:50:22] [TRT] [I] [MemUsageChange] Init CUDA: CPU +13, GPU +0, now: CPU 118, GPU 423 (MiB)\n",
      "[06/18/2024-11:50:27] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +1974, GPU +350, now: CPU 2228, GPU 773 (MiB)\n",
      "[06/18/2024-11:50:27] [TRT-LLM] [W] Invalid timing cache, using freshly created one\n",
      "[06/18/2024-11:50:32] [TRT-LLM] [I] Loading HF LLaMA ... from ./hf-weights/7B-chat\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:03<00:00,  1.99s/it]\n",
      "[06/18/2024-11:50:54] [TRT-LLM] [I] HF LLaMA loaded. Total time: 00:00:22\n",
      "[06/18/2024-11:50:54] [TRT-LLM] [I] Loading weights from HF LLaMA...\n",
      "[06/18/2024-11:51:30] [TRT-LLM] [I] Weights loaded. Total time: 00:00:35\n",
      "[06/18/2024-11:51:30] [TRT-LLM] [I] Context FMHA Enabled\n",
      "[06/18/2024-11:51:30] [TRT-LLM] [I] Remove Padding Enabled\n",
      "[06/18/2024-11:51:30] [TRT-LLM] [I] Paged KV Cache Enabled\n",
      "[06/18/2024-11:51:30] [TRT-LLM] [I] Build TensorRT engine llama_float16_tp1_rank0.engine\n",
      "[06/18/2024-11:51:30] [TRT] [W] Unused Input: position_ids\n",
      "[06/18/2024-11:51:30] [TRT] [W] [RemoveDeadLayers] Input Tensor position_ids is unused or used only at compile-time, but is not being removed.\n",
      "[06/18/2024-11:51:30] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 28026, GPU 1369 (MiB)\n",
      "[06/18/2024-11:51:30] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +2, GPU +10, now: CPU 28028, GPU 1379 (MiB)\n",
      "[06/18/2024-11:51:30] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.\n",
      "[06/18/2024-11:51:51] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n",
      "[06/18/2024-11:51:51] [TRT] [I] Detected 41 inputs and 1 output network tensors.\n",
      "[06/18/2024-11:52:00] [TRT] [I] Total Host Persistent Memory: 90016\n",
      "[06/18/2024-11:52:00] [TRT] [I] Total Device Persistent Memory: 0\n",
      "[06/18/2024-11:52:00] [TRT] [I] Total Scratch Memory: 33620096\n",
      "[06/18/2024-11:52:00] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 555 steps to complete.\n",
      "[06/18/2024-11:52:00] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 16.4953ms to assign 11 blocks to 555 nodes requiring 1291848704 bytes.\n",
      "[06/18/2024-11:52:00] [TRT] [I] Total Activation Memory: 1291848704\n",
      "[06/18/2024-11:52:00] [TRT] [I] Total Weights Memory: 13476831256\n",
      "[06/18/2024-11:52:00] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 28342, GPU 14247 (MiB)\n",
      "[06/18/2024-11:52:00] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +10, now: CPU 28342, GPU 14257 (MiB)\n",
      "[06/18/2024-11:52:00] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 500 MiB, GPU 66048 MiB\n",
      "[06/18/2024-11:52:00] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +12853, now: CPU 0, GPU 12853 (MiB)\n",
      "[06/18/2024-11:52:05] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 41520 MiB\n",
      "[06/18/2024-11:52:05] [TRT-LLM] [I] Total time of building llama_float16_tp1_rank0.engine: 00:00:35\n",
      "[06/18/2024-11:52:05] [TRT-LLM] [I] Config saved to ./trt-engines/llama_7b/fp16/1-gpu/config.json.\n",
      "[06/18/2024-11:52:05] [TRT-LLM] [I] Serializing engine to ./trt-engines/llama_7b/fp16/1-gpu/llama_float16_tp1_rank0.engine...\n",
      "[06/18/2024-11:52:17] [TRT-LLM] [I] Engine serialized. Total time: 00:00:11\n",
      "[06/18/2024-11:52:17] [TRT-LLM] [I] Timing cache serialized to ./trt-engines/llama_7b/fp16/1-gpu/model.cache\n",
      "[06/18/2024-11:52:17] [TRT-LLM] [I] Total time of building all 1 engines: 00:01:55\n"
     ]
    }
   ],
   "source": [
    "!python TensorRT-LLM/examples/llama/build.py \\\n",
    "    --model_dir ./hf-weights/7B-chat  \\\n",
    "    --dtype float16 \\\n",
    "    --use_gpt_attention_plugin float16  \\\n",
    "    --paged_kv_cache \\\n",
    "    --remove_input_padding \\\n",
    "    --use_gemm_plugin float16  \\\n",
    "    --output_dir \"./trt-engines/llama_7b/fp16/1-gpu\"  \\\n",
    "    --max_input_len 2048 --max_output_len 512 \\\n",
    "    --use_rmsnorm_plugin float16  \\\n",
    "    --enable_context_fmha \\\n",
    "    --use_inflight_batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d970074-4ec6-4dd8-ba05-b4792de1b4ef",
   "metadata": {},
   "source": [
    "Now let's run the inference of the llama-2-7b-chat model. Similarly, TensorRT-LLM provides APIs to do that. In this example, the inference script `TensorRT-LLM/examples/llama/run.py` is provided by TensorRT-LLM repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fbb11ea-b75e-43b0-9bb0-ff9a247d741b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the float16 engine ...\n",
      "Input: \"Let me explain what DNA is. DNA, which stands for\"\n",
      "Output: \" deoxyribonucleic acid, is a molecule that contains the genetic instructions used in the development and function of all living organisms. It is a long, complex molecule that is made up of four different chemical bases: adenine (A), guanine (G), cytosine (C), and thymine (T). These bases are arranged in a specific sequence, creating a unique code that determines the characteristics of an organism.\n",
      "The sequence of these bases along a DNA molecule determines the genetic information encoded in the DNA. This genetic information is used in the development and function of cells, tissues, and organs, and it is passed from one generation to the next through the replication of DNA.\n",
      "DNA is found in the nucleus of eukaryotic cells (such as humans) and in the cytoplasm of prokaryotic cells (such as bacteria). It is a double-stranded molecule, meaning that there are two complementary strands of nucleotides that are held together by hydrogen bonds between the bases.\n",
      "The structure of DNA is important for its function because it allows the molecule to be replicated and transcribed into RNA, which is then translated into proteins. The sequence of bases in DNA also determines the genetic information encoded in the molecule, which is used to create proteins that perform specific functions in the cell.\n",
      "In addition to its role in genetics, DNA is also important in forensic science. DNA analysis is used to identify individuals and solve crimes by comparing the DNA found at a crime scene with the DNA of potential suspects. This can be done through techniques such as PCR (polymerase chain reaction), which allows for the amplification of specific DNA sequences, and DNA sequencing, which determines the order of the bases in a DNA molecule.\n",
      "Overall, DNA is a fascinating molecule that plays a crucial role in the function and development of living organisms. Its unique structure and function make it an important tool in forensic science, and its study has led to many important discoveries in genetics and molecular biology.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\"\n"
     ]
    }
   ],
   "source": [
    "!python TensorRT-LLM/examples/llama/run.py \\\n",
    "    --engine_dir=./trt-engines/llama_7b/fp16/1-gpu \\\n",
    "    --max_output_len 500 \\\n",
    "    --tokenizer_dir \"llama-models\" \\\n",
    "    --input_text \"Let me explain what DNA is. DNA, which stands for\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5a4c5e-0a95-442b-8bf7-d230a853efb8",
   "metadata": {},
   "source": [
    "## 3. Setup Triton Inference Server for LLM inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d37d31-9526-44eb-87ec-308948864735",
   "metadata": {},
   "source": [
    "To start with Triton, a model repository with certain structure and configuration files should be prepared first. For ease of simplicity, all is setup already in the `triton_model_repo` folder in this example.\n",
    "\n",
    "Here we will setup 2 LLM inference pipeline: the vanilla Pytorch pipeline with optimization, and the optimized TensorRT-LLM pipeline for the llama-2-7b-chat model.\n",
    "- The Python pipeline uses huggingface APIs. The model repo is located at `./triton_model_repo/llama_7b/python/llama-huggingface`\n",
    "- The TensorRT-LLM pipeline contains multiple separated components under `./triton_model_repo/llama_7b/python`: `preprocessing`, `tensorrt_llm` and `postprocessing`. Here we created an `ensemble` folder which encapsulates the `preprocessing`, `postprocessing` and `tensorrt_llm` steps in the same folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ccac43a-9028-4047-8b35-77237e06e0d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1581 B]\n",
      "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]      \u001b[0m\u001b[33m\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]                \u001b[0m\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]        \u001b[33m\n",
      "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [927 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]      \u001b[0m\u001b[33m\u001b[33m\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [51.8 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2187 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2546 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1392 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [31.8 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [81.0 kB]\n",
      "Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1917 kB]\n",
      "Get:18 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2469 kB]33m\n",
      "Get:19 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.7 kB]\n",
      "Get:20 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1092 kB]\n",
      "Fetched 33.1 MB in 2s (16.5 MB/s)33m                        \u001b[0m\u001b[33m\u001b[33m\u001b[33m\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "103 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  tree\n",
      "0 upgraded, 1 newly installed, 0 to remove and 103 not upgraded.\n",
      "Need to get 47.9 kB of archives.\n",
      "After this operation, 116 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tree amd64 2.0.2-1 [47.9 kB]\n",
      "Fetched 47.9 kB in 0s (141 kB/s)m\u001b[33m\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package tree.\n",
      "(Reading database ... 25642 files and directories currently installed.)\n",
      "Preparing to unpack .../tree_2.0.2-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Unpacking tree (2.0.2-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 40%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Setting up tree (2.0.2-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 60%]\u001b[49m\u001b[39m [##################################........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [##############################################............] \u001b8\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J"
     ]
    }
   ],
   "source": [
    "!apt update && apt install tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3efd46e1-439c-482c-888c-52b999249f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./triton_model_repo/\u001b[0m\n",
      "└── \u001b[01;34mllama_7b\u001b[0m\n",
      "    └── \u001b[01;34mpython\u001b[0m\n",
      "        ├── \u001b[01;34mensemble\u001b[0m\n",
      "        │   ├── \u001b[01;34m1\u001b[0m\n",
      "        │   └── \u001b[01;32mconfig.pbtxt\u001b[0m\n",
      "        ├── \u001b[01;34mllama_huggingface\u001b[0m\n",
      "        │   ├── \u001b[01;34m1\u001b[0m\n",
      "        │   │   ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "        │   │   │   └── \u001b[00mmodel.cpython-310.pyc\u001b[0m\n",
      "        │   │   └── \u001b[00mmodel.py\u001b[0m\n",
      "        │   └── \u001b[00mconfig.pbtxt\u001b[0m\n",
      "        ├── \u001b[01;34mpostprocessing\u001b[0m\n",
      "        │   ├── \u001b[01;34m1\u001b[0m\n",
      "        │   │   ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "        │   │   │   └── \u001b[00mmodel.cpython-310.pyc\u001b[0m\n",
      "        │   │   └── \u001b[00mmodel.py\u001b[0m\n",
      "        │   └── \u001b[01;32mconfig.pbtxt\u001b[0m\n",
      "        ├── \u001b[01;34mpreprocessing\u001b[0m\n",
      "        │   ├── \u001b[01;34m1\u001b[0m\n",
      "        │   │   ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "        │   │   │   └── \u001b[00mmodel.cpython-310.pyc\u001b[0m\n",
      "        │   │   └── \u001b[00mmodel.py\u001b[0m\n",
      "        │   └── \u001b[00mconfig.pbtxt\u001b[0m\n",
      "        └── \u001b[01;34mtensorrt_llm\u001b[0m\n",
      "            ├── \u001b[01;34m1\u001b[0m\n",
      "            └── \u001b[00mconfig.pbtxt\u001b[0m\n",
      "\n",
      "15 directories, 11 files\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p triton_model_repo/llama_7b/python/ensemble/1\n",
    "!tree ./triton_model_repo/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab3ad7e-a901-44d5-ac05-fcb28f40923e",
   "metadata": {},
   "source": [
    "Feel free to look at the `config.pbtxt` files in each component folder to understand how Triton configures the inference pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eafa798-7004-4a50-86a2-2e7c8a523bc2",
   "metadata": {},
   "source": [
    "Now we can start the Triton server to serve the 2 pipelines\n",
    "\n",
    "Note: \n",
    "- **Start a separate terminal and run the following commands in terminal**\n",
    "- **Make sure that you do not have `.ipynb_checkpoints` under `triton_model_repo/llama_7b/python`, this folder can be auto-generated by jupyter and can mess up the launching of Triton.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7f08a31-a55c-42ba-968d-c4b119fc5198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find: ‘./.ipynb_checkpoints’: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "## Remove ./triton_model_repo/llama_7b/python/.ipynb_checkpoints, which can mess up launch of triton\n",
    "!rm -rf ./triton_model_repo/llama_7b/python/.ipynb_checkpoints\n",
    "!find . -type d -name \".ipynb_checkpoints\" -exec rm -rf {} \\;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d857614-0115-4bbb-b876-48bac723e931",
   "metadata": {},
   "source": [
    "**Note: launch the command in the following cell in a separate terminal - this server command needs to be kept alive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89ce456f-084b-4159-b072-981866950f30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0610 13:45:52.600096 2331 pinned_memory_manager.cc:241] Pinned memory pool is created at '0x7fa74c000000' with size 268435456\n",
      "I0610 13:45:52.600852 2331 cuda_memory_manager.cc:107] CUDA memory pool is created on device 0 with size 67108864\n",
      "I0610 13:45:52.625729 2331 model_lifecycle.cc:461] loading: postprocessing:1\n",
      "I0610 13:45:52.626264 2331 model_lifecycle.cc:461] loading: preprocessing:1\n",
      "I0610 13:45:52.626789 2331 model_lifecycle.cc:461] loading: tensorrt_llm:1\n",
      "I0610 13:45:52.627317 2331 model_lifecycle.cc:461] loading: llama_huggingface:1\n",
      "E0610 13:45:52.690018 2331 backend_model.cc:634] ERROR: Failed to create instance: unexpected error when creating modelInstanceState: [json.exception.parse_error.101] parse error at line 1, column 1: syntax error while parsing value - unexpected end of input; expected '[', '{', or a literal\n",
      "E0610 13:45:52.690081 2331 model_lifecycle.cc:621] failed to load 'tensorrt_llm' version 1: Internal: unexpected error when creating modelInstanceState: [json.exception.parse_error.101] parse error at line 1, column 1: syntax error while parsing value - unexpected end of input; expected '[', '{', or a literal\n",
      "I0610 13:45:52.690097 2331 model_lifecycle.cc:756] failed to load 'tensorrt_llm'\n",
      "I0610 13:45:54.549493 2331 python_be.cc:2199] TRITONBACKEND_ModelInstanceInitialize: postprocessing_0_0 (CPU device 0)\n",
      "I0610 13:45:55.028709 2331 pb_stub.cc:325] Failed to initialize Python stub: HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/workspace/notebooks/tensorrt-llm/llama-models'. Use `repo_type` argument if needed.\n",
      "\n",
      "At:\n",
      "  /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py(158): validate_repo_id\n",
      "  /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py(110): _inner_fn\n",
      "  /usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py(467): cached_file\n",
      "  /usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py(1800): from_pretrained\n",
      "  /workspace/nv-inference-demo/notebooks/tensorrt-llm/triton_model_repo/llama_7b/python/postprocessing/1/model.py(68): initialize\n",
      "\n",
      "E0610 13:45:55.240633 2331 backend_model.cc:634] ERROR: Failed to create instance: HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/workspace/notebooks/tensorrt-llm/llama-models'. Use `repo_type` argument if needed.\n",
      "\n",
      "At:\n",
      "  /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py(158): validate_repo_id\n",
      "  /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py(110): _inner_fn\n",
      "  /usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py(467): cached_file\n",
      "  /usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py(1800): from_pretrained\n",
      "  /workspace/nv-inference-demo/notebooks/tensorrt-llm/triton_model_repo/llama_7b/python/postprocessing/1/model.py(68): initialize\n",
      "\n",
      "E0610 13:45:55.240707 2331 model_lifecycle.cc:621] failed to load 'postprocessing' version 1: Internal: HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/workspace/notebooks/tensorrt-llm/llama-models'. Use `repo_type` argument if needed.\n",
      "\n",
      "At:\n",
      "  /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py(158): validate_repo_id\n",
      "  /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py(110): _inner_fn\n",
      "  /usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py(467): cached_file\n",
      "  /usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py(1800): from_pretrained\n",
      "  /workspace/nv-inference-demo/notebooks/tensorrt-llm/triton_model_repo/llama_7b/python/postprocessing/1/model.py(68): initialize\n",
      "\n",
      "I0610 13:45:55.240734 2331 model_lifecycle.cc:756] failed to load 'postprocessing'\n",
      "I0610 13:45:55.709242 2331 python_be.cc:2199] TRITONBACKEND_ModelInstanceInitialize: llama_huggingface_0_0 (GPU device 0)\n",
      "I0610 13:45:56.016919 2331 python_be.cc:2199] TRITONBACKEND_ModelInstanceInitialize: preprocessing_0_0 (CPU device 0)\n",
      "I0610 13:45:57.343417 2331 model.py:59] Loading HuggingFace model: /workspace/notebooks/tensorrt-llm/hf-weights/7B-chat...\n",
      "I0610 13:45:57.451731 2331 pb_stub.cc:325] Failed to initialize Python stub: HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/workspace/notebooks/tensorrt-llm/hf-weights/7B-chat'. Use `repo_type` argument if needed.\n",
      "\n",
      "At:\n",
      "  /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py(158): validate_repo_id\n",
      "  /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py(110): _inner_fn\n",
      "  /usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py(467): cached_file\n",
      "  /usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py(496): get_tokenizer_config\n",
      "  /usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py(652): from_pretrained\n",
      "  /workspace/nv-inference-demo/notebooks/tensorrt-llm/triton_model_repo/llama_7b/python/llama_huggingface/1/model.py(61): initialize\n",
      "\n",
      "I0610 13:45:57.660246 2331 pb_stub.cc:325] Failed to initialize Python stub: HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/workspace/notebooks/tensorrt-llm/llama-models'. Use `repo_type` argument if needed.\n",
      "\n",
      "At:\n",
      "  /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py(158): validate_repo_id\n",
      "  /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py(110): _inner_fn\n",
      "  /usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py(467): cached_file\n",
      "  /usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py(1800): from_pretrained\n",
      "  /workspace/nv-inference-demo/notebooks/tensorrt-llm/triton_model_repo/llama_7b/python/preprocessing/1/model.py(72): initialize\n",
      "\n",
      "E0610 13:45:57.831090 2331 backend_model.cc:634] ERROR: Failed to create instance: HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/workspace/notebooks/tensorrt-llm/hf-weights/7B-chat'. Use `repo_type` argument if needed.\n",
      "\n",
      "At:\n",
      "  /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py(158): validate_repo_id\n",
      "  /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py(110): _inner_fn\n",
      "  /usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py(467): cached_file\n",
      "  /usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py(496): get_tokenizer_config\n",
      "  /usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py(652): from_pretrained\n",
      "  /workspace/nv-inference-demo/notebooks/tensorrt-llm/triton_model_repo/llama_7b/python/llama_huggingface/1/model.py(61): initialize\n",
      "\n",
      "E0610 13:45:57.831234 2331 model_lifecycle.cc:621] failed to load 'llama_huggingface' version 1: Internal: HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/workspace/notebooks/tensorrt-llm/hf-weights/7B-chat'. Use `repo_type` argument if needed.\n",
      "\n",
      "At:\n",
      "  /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py(158): validate_repo_id\n",
      "  /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py(110): _inner_fn\n",
      "  /usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py(467): cached_file\n",
      "  /usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py(496): get_tokenizer_config\n",
      "  /usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py(652): from_pretrained\n",
      "  /workspace/nv-inference-demo/notebooks/tensorrt-llm/triton_model_repo/llama_7b/python/llama_huggingface/1/model.py(61): initialize\n",
      "\n",
      "I0610 13:45:57.831278 2331 model_lifecycle.cc:756] failed to load 'llama_huggingface'\n",
      "E0610 13:45:58.139524 2331 backend_model.cc:634] ERROR: Failed to create instance: HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/workspace/notebooks/tensorrt-llm/llama-models'. Use `repo_type` argument if needed.\n",
      "\n",
      "At:\n",
      "  /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py(158): validate_repo_id\n",
      "  /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py(110): _inner_fn\n",
      "  /usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py(467): cached_file\n",
      "  /usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py(1800): from_pretrained\n",
      "  /workspace/nv-inference-demo/notebooks/tensorrt-llm/triton_model_repo/llama_7b/python/preprocessing/1/model.py(72): initialize\n",
      "\n",
      "E0610 13:45:58.139622 2331 model_lifecycle.cc:621] failed to load 'preprocessing' version 1: Internal: HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/workspace/notebooks/tensorrt-llm/llama-models'. Use `repo_type` argument if needed.\n",
      "\n",
      "At:\n",
      "  /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py(158): validate_repo_id\n",
      "  /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py(110): _inner_fn\n",
      "  /usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py(467): cached_file\n",
      "  /usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py(1800): from_pretrained\n",
      "  /workspace/nv-inference-demo/notebooks/tensorrt-llm/triton_model_repo/llama_7b/python/preprocessing/1/model.py(72): initialize\n",
      "\n",
      "I0610 13:45:58.139661 2331 model_lifecycle.cc:756] failed to load 'preprocessing'\n",
      "E0610 13:45:58.139758 2331 model_repository_manager.cc:563] Invalid argument: ensemble 'ensemble' depends on 'postprocessing' which has no loaded version. Model 'postprocessing' loading failed with error: version 1 is at UNAVAILABLE state: Internal: HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/workspace/notebooks/tensorrt-llm/llama-models'. Use `repo_type` argument if needed.\n",
      "\n",
      "At:\n",
      "  /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py(158): validate_repo_id\n",
      "  /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py(110): _inner_fn\n",
      "  /usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py(467): cached_file\n",
      "  /usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py(1800): from_pretrained\n",
      "  /workspace/nv-inference-demo/notebooks/tensorrt-llm/triton_model_repo/llama_7b/python/postprocessing/1/model.py(68): initialize\n",
      ";\n",
      "I0610 13:45:58.139875 2331 server.cc:592] \n",
      "+------------------+------+\n",
      "| Repository Agent | Path |\n",
      "+------------------+------+\n",
      "+------------------+------+\n",
      "\n",
      "I0610 13:45:58.139917 2331 server.cc:619] \n",
      "+-------------+-------------------------------+-------------------------------+\n",
      "| Backend     | Path                          | Config                        |\n",
      "+-------------+-------------------------------+-------------------------------+\n",
      "| python      | /opt/tritonserver/backends/py | {\"cmdline\":{\"auto-complete-co |\n",
      "|             | thon/libtriton_python.so      | nfig\":\"true\",\"backend-directo |\n",
      "|             |                               | ry\":\"/opt/tritonserver/backen |\n",
      "|             |                               | ds\",\"min-compute-capability\": |\n",
      "|             |                               | \"6.000000\",\"default-max-batch |\n",
      "|             |                               | -size\":\"4\"}}                  |\n",
      "|             |                               |                               |\n",
      "| tensorrtllm | /opt/tritonserver/backends/te | {\"cmdline\":{\"auto-complete-co |\n",
      "|             | nsorrtllm/libtriton_tensorrtl | nfig\":\"true\",\"backend-directo |\n",
      "|             | lm.so                         | ry\":\"/opt/tritonserver/backen |\n",
      "|             |                               | ds\",\"min-compute-capability\": |\n",
      "|             |                               | \"6.000000\",\"default-max-batch |\n",
      "|             |                               | -size\":\"4\"}}                  |\n",
      "|             |                               |                               |\n",
      "|             |                               |                               |\n",
      "+-------------+-------------------------------+-------------------------------+\n",
      "\n",
      "I0610 13:45:58.139991 2331 server.cc:662] \n",
      "+-------------------+---------+-----------------------------------------------+\n",
      "| Model             | Version | Status                                        |\n",
      "+-------------------+---------+-----------------------------------------------+\n",
      "| llama_huggingface | 1       | UNAVAILABLE: Internal: HFValidationError: Rep |\n",
      "|                   |         | o id must be in the form 'repo_name' or 'name |\n",
      "|                   |         | space/repo_name': '/workspace/notebooks/tenso |\n",
      "|                   |         | rrt-llm/hf-weights/7B-chat'. Use `repo_type`  |\n",
      "|                   |         | argument if needed.                           |\n",
      "|                   |         |   /usr/local/lib/python3.10/dist-packages/tra |\n",
      "|                   |         | nsformers/utils/hub.py(467): cached_file      |\n",
      "|                   |         |   /usr/local/lib/python3.10/dist-packages/tra |\n",
      "|                   |         | nsformers/models/auto/tokenization_auto.py(65 |\n",
      "|                   |         | 2): from_pretrained                           |\n",
      "|                   |         |   /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py(158): validate_repo_id |\n",
      "|                   |         |   /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py(110): _inner_fn |\n",
      "|                   |         |   /usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py(467): cached_file |\n",
      "| postprocessing    | 1       | UNAVAILABLE: Internal: HFValidationError: Rep |\n",
      "|                   |         | o id must be in the form 'repo_name' or 'name |\n",
      "|                   |         | space/repo_name': '/workspace/notebooks/tenso |\n",
      "|                   |         | rrt-llm/llama-models'. Use `repo_type` argume |\n",
      "|                   |         | nt if needed.                                 |\n",
      "|                   |         |   /usr/local/lib/python3.10/dist-packages/tra |\n",
      "|                   |         | nsformers/utils/hub.py(467): cached_file      |\n",
      "|                   |         |   /workspace/nv-inference-demo/notebooks/tens |\n",
      "|                   |         | orrt-llm/triton_model_repo/llama_7b/python/po |\n",
      "|                   |         | stprocessing/1/model.py(68): initialize       |\n",
      "|                   |         |   /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py(158): validate_repo_id |\n",
      "|                   |         |   /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py(110): _inner_fn |\n",
      "| preprocessing     | 1       | UNAVAILABLE: Internal: HFValidationError: Rep |\n",
      "|                   |         | o id must be in the form 'repo_name' or 'name |\n",
      "|                   |         | space/repo_name': '/workspace/notebooks/tenso |\n",
      "|                   |         | rrt-llm/llama-models'. Use `repo_type` argume |\n",
      "|                   |         | nt if needed.                                 |\n",
      "|                   |         |   /usr/local/lib/python3.10/dist-packages/tra |\n",
      "|                   |         | nsformers/utils/hub.py(467): cached_file      |\n",
      "|                   |         |   /workspace/nv-inference-demo/notebooks/tens |\n",
      "|                   |         | orrt-llm/triton_model_repo/llama_7b/python/pr |\n",
      "|                   |         | eprocessing/1/model.py(72): initialize        |\n",
      "|                   |         |   /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py(158): validate_repo_id |\n",
      "|                   |         |   /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py(110): _inner_fn |\n",
      "| tensorrt_llm      | 1       | UNAVAILABLE: Internal: unexpected error when  |\n",
      "|                   |         | creating modelInstanceState: [json.exception. |\n",
      "|                   |         | parse_error.101] parse error at line 1, colum |\n",
      "|                   |         | n 1: syntax error while parsing value - unexp |\n",
      "|                   |         | ected end of input; expected '[', '{', or a l |\n",
      "|                   |         | iteral                                        |\n",
      "+-------------------+---------+-----------------------------------------------+\n",
      "\n",
      "I0610 13:45:58.248475 2331 metrics.cc:817] Collecting metrics for GPU 0: NVIDIA A100-SXM4-80GB\n",
      "I0610 13:45:58.249365 2331 metrics.cc:710] Collecting CPU metrics\n",
      "I0610 13:45:58.249510 2331 tritonserver.cc:2458] \n",
      "+----------------------------------+------------------------------------------+\n",
      "| Option                           | Value                                    |\n",
      "+----------------------------------+------------------------------------------+\n",
      "| server_id                        | triton                                   |\n",
      "| server_version                   | 2.39.0                                   |\n",
      "| server_extensions                | classification sequence model_repository |\n",
      "|                                  |  model_repository(unload_dependents) sch |\n",
      "|                                  | edule_policy model_configuration system_ |\n",
      "|                                  | shared_memory cuda_shared_memory binary_ |\n",
      "|                                  | tensor_data parameters statistics trace  |\n",
      "|                                  | logging                                  |\n",
      "| model_repository_path[0]         | /workspace/nv-inference-demo/notebooks/t |\n",
      "|                                  | ensorrt-llm/triton_model_repo/llama_7b/p |\n",
      "|                                  | ython                                    |\n",
      "| model_control_mode               | MODE_NONE                                |\n",
      "| strict_model_config              | 0                                        |\n",
      "| rate_limit                       | OFF                                      |\n",
      "| pinned_memory_pool_byte_size     | 268435456                                |\n",
      "| cuda_memory_pool_byte_size{0}    | 67108864                                 |\n",
      "| min_supported_compute_capability | 6.0                                      |\n",
      "| strict_readiness                 | 1                                        |\n",
      "| exit_timeout                     | 30                                       |\n",
      "| cache_enabled                    | 0                                        |\n",
      "+----------------------------------+------------------------------------------+\n",
      "\n",
      "I0610 13:45:58.249531 2331 server.cc:293] Waiting for in-flight requests to complete.\n",
      "I0610 13:45:58.249535 2331 server.cc:309] Timeout 30: Found 0 model versions that have in-flight inferences\n",
      "I0610 13:45:58.249542 2331 server.cc:324] All models are stopped, unloading models\n",
      "I0610 13:45:58.249545 2331 server.cc:331] Timeout 30: Found 0 live models and 0 in-flight non-inference requests\n",
      "error: creating server: Internal - failed to load all models\n"
     ]
    }
   ],
   "source": [
    "!tritonserver --model-repository=/workspace/nv-inference-demo/notebooks/tensorrt-llm/triton_model_repo/llama_7b/python # --log-verbose 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82c5d81-d2e9-4338-8c47-38d4d43a3b2c",
   "metadata": {},
   "source": [
    "You can verify that the triton server has successfully launch when you see terminal output such as below:\n",
    "```\n",
    "\n",
    "I0304 16:11:54.555571 7346 server.cc:662] \n",
    "+-------------------+---------+--------+\n",
    "| Model             | Version | Status |\n",
    "+-------------------+---------+--------+\n",
    "| ensemble          | 1       | READY  |\n",
    "| llama_huggingface | 1       | READY  |\n",
    "| postprocessing    | 1       | READY  |\n",
    "| preprocessing     | 1       | READY  |\n",
    "| tensorrt_llm      | 1       | READY  |\n",
    "+-------------------+---------+--------+\n",
    "\n",
    "I0304 16:11:54.593570 7346 metrics.cc:817] Collecting metrics for GPU 0: NVIDIA A100-SXM4-80GB\n",
    "I0304 16:11:54.594500 7346 metrics.cc:710] Collecting CPU metrics\n",
    "I0304 16:11:54.594653 7346 tritonserver.cc:2458] \n",
    "+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
    "| Option                           | Value                                                                                                                                                                                                           |\n",
    "+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
    "| server_id                        | triton                                                                                                                                                                                                          |\n",
    "| server_version                   | 2.39.0                                                                                                                                                                                                          |\n",
    "| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data parameters statistics trace logging |\n",
    "| model_repository_path[0]         | /workspace/notebooks/tensorrt-llm/triton_model_repo/llama_7b/python                                                                                                                                             |\n",
    "| model_control_mode               | MODE_NONE                                                                                                                                                                                                       |\n",
    "| strict_model_config              | 0                                                                                                                                                                                                               |\n",
    "| rate_limit                       | OFF                                                                                                                                                                                                             |\n",
    "| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                                       |\n",
    "| cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                                                                                                        |\n",
    "| min_supported_compute_capability | 6.0                                                                                                                                                                                                             |\n",
    "| strict_readiness                 | 1                                                                                                                                                                                                               |\n",
    "| exit_timeout                     | 30                                                                                                                                                                                                              |\n",
    "| cache_enabled                    | 0                                                                                                                                                                                                               |\n",
    "+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
    "\n",
    "I0304 16:11:54.596304 7346 grpc_server.cc:2513] Started GRPCInferenceService at 0.0.0.0:8001\n",
    "I0304 16:11:54.596487 7346 http_server.cc:4497] Started HTTPService at 0.0.0.0:8000\n",
    "I0304 16:11:54.637582 7346 http_server.cc:270] Started Metrics Service at 0.0.0.0:8002\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5155bd83-31c7-468e-a89d-1fd2f8d24085",
   "metadata": {},
   "source": [
    "Now let's send inference requests to the triton server via triton client. To send an inflight inference request to Triton server, launch the following command using the provided client side script.\n",
    "\n",
    "Here we are sending request to the Python pipeline `llama_huggingface`, feel free to change `--model_name` to `ensemble` to send request to tensorRT-LLM pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7d82cfc-6a3d-4a2f-b4c1-865d5ee28143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Here is an explanation of what DNA is:\\n\\nDNA (Deoxyribonucleic acid) is a molecule that contains the genetic instructions used in the development and function of all living organisms. DNA is a long, double-stranded helix made up of nucleotides, which are the building blocks of DNA. Each nucleotide is composed of a sugar molecule called deoxyribose, a phosphate group, and one of four nitrogenous bases - adenine (A), guanine (G), cytosine (C), and thymine (T). The sequence of these nitrogenous bases along the DNA molecule determines the genetic information encoded in the DNA.\\nDNA is found in the nucleus of eukaryotic cells (such as humans) and in prokaryotic cells (such as bacteria). It is the primary source of genetic information that is passed from one generation to the next, and it plays a central role in the development and function of all living organisms.\\nIn summary, DNA is a molecule that contains the genetic instructions used in the development and function of all living organisms. It is a long, double-stranded helix made up of nucleotides, and the sequence of these nucleotides determines the genetic information encoded in the DNA.'\n"
     ]
    }
   ],
   "source": [
    "!python ./end_to_end_streaming_client.py -u localhost:8001 --model_name llama_huggingface --max_tokens 500  --prompt \"Here is an explanation of what DNA is:\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936ac9f7-39c3-4b16-8f31-4b512615a445",
   "metadata": {},
   "source": [
    "## 3. Benchmark Python pipeline vs TensorRT-LLM pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53c02bf-8e21-4cfc-9431-75d3ae967b78",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Now we are ready to benchmark the performance of TensorRT-LLM for llama-2-7b-chat inference vs the Python pipeline. A benchmark script `identity_test_python_vs_trtllm.py` is provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30a4221-c004-48e3-bf85-c0cb2f30b1a0",
   "metadata": {},
   "source": [
    "Run the following command to benchmark the throughput of the huggingface Python pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5798da34-f702-4492-8649-766e889bf6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Warm up for benchmarking.\n",
      "[INFO] Start benchmarking on 4 prompts.\n",
      "[INFO] Total Latency: 3764.447 ms\n"
     ]
    }
   ],
   "source": [
    "!python ./identity_test_python_vs_trtllm.py \\\n",
    "    -u localhost:8001 \\\n",
    "    --max_input_len 100 \\\n",
    "    --dataset /workspace/nv-inference-demo/notebooks/tensorrt-llm/datasets/mini_cnn_eval.json \\\n",
    "    -i grpc \\\n",
    "    --model_name \"llama_huggingface\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9ce329-ecd2-42bb-a0db-0ea90822dba5",
   "metadata": {},
   "source": [
    "Run the following command to benchmark the throughput of the tensorRT-LLM pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "991f5b35-bf49-4994-aba0-ce0e945a1f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Warm up for benchmarking.\n",
      "[INFO] Start benchmarking on 4 prompts.\n",
      "[INFO] Total Latency: 722.567 ms\n"
     ]
    }
   ],
   "source": [
    "!python ./identity_test_python_vs_trtllm.py \\\n",
    "    -u localhost:8001 \\\n",
    "    --max_input_len 100 \\\n",
    "    --dataset /workspace/nv-inference-demo/notebooks/tensorrt-llm/datasets/mini_cnn_eval.json \\\n",
    "    -i grpc \\\n",
    "    --model_name \"ensemble\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49998a37-dc51-44f6-a3a2-77f531c839c8",
   "metadata": {},
   "source": [
    "Depending on the type of GPU you are using, you can observe a different factor of speed-up for the latency measurement, typically around 4 - 5x."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28742735-34bd-49c5-81ef-7e8c363d6402",
   "metadata": {},
   "source": [
    "## 4. Going further\n",
    "\n",
    "We are not finished with TensorRT optimization yet, we can further push the optimization much further with techniques such as:\n",
    "- Parallelisation: pipeline and tensor\n",
    "- Inflight dynamic batching\n",
    "- Model quantization\n",
    "\n",
    "We will not cover these in this tutorial, but feel free to explore & test these optimizations by referring to the original demo [here](https://github.com/scaleway/ai-pulse-nvidia-trt-llm/tree/main/docs)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
