{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7629d307",
   "metadata": {},
   "source": [
    "# Getting started with TensorRT-LLM and Triton Inference Server\n",
    "\n",
    "This hands-on tutorial is based on the TensorRT-LLM demo from ai-PULSE by Scaleway, which can be found here: https://github.com/scaleway/ai-pulse-nvidia-trt-llm/tree/main\n",
    "\n",
    "In this tutorial, we will cover\n",
    "- How to convert llama 2 models to TensorRT-LLM format\n",
    "- Set-up Triton Inference Server with llama 2 models optimized using TensorRT-LLM\n",
    "- Benchmark the inference performance of Triton + TensorRT-LLM pipeline vs vanilla Python HuggingFace pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc83b427",
   "metadata": {},
   "source": [
    "## 1. Setup the environment\n",
    "\n",
    "First let's clone the TensorRT-LLM github repo and be sure to use the correct version for this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db9b6b95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'TensorRT-LLM'...\n",
      "remote: Enumerating objects: 10247, done.\u001b[K\n",
      "remote: Counting objects: 100% (207/207), done.\u001b[K\n",
      "remote: Compressing objects: 100% (124/124), done.\u001b[K\n",
      "remote: Total 10247 (delta 64), reused 192 (delta 61), pack-reused 10040\u001b[K\n",
      "Receiving objects: 100% (10247/10247), 130.76 MiB | 37.75 MiB/s, done.\n",
      "Resolving deltas: 100% (7052/7052), done.\n",
      "Updating files: 100% (1949/1949), done.\n",
      "Updating files: 100% (1725/1725), done.\n",
      "Note: switching to 'v0.5.0'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by switching back to a branch.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -c with the switch command. Example:\n",
      "\n",
      "  git switch -c <new-branch-name>\n",
      "\n",
      "Or undo this operation with:\n",
      "\n",
      "  git switch -\n",
      "\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\n",
      "\n",
      "HEAD is now at ffd5af3 revise the homepage (#14)\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/NVIDIA/TensorRT-LLM.git \n",
    "!git config --global --add safe.directory /workspace/notebooks/tensorrt-llm/TensorRT-LLM\n",
    "!cd TensorRT-LLM && git checkout v0.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70f846d",
   "metadata": {},
   "source": [
    "Next, let's download the llama 2 models, if it is not already done yet. \n",
    "\n",
    "For this you need to go to the models [website](https://llama.meta.com/llama-downloads), register, then an email with a custom URL will be sent to you allowing you to download the llama models.\n",
    "\n",
    "To proceed with the download, first clone the llama repo, then launch the download script. When prompt with URL, just enter the URL that you received via email before. For this tutorial, we will need to download 1 model: the 7B-chat. Put the downloaded model inside `./llama-models` folder.\n",
    "\n",
    "Note: **the download could take long time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84a9ef19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'llama' already exists and is not an empty directory.\n",
      "total 524K\n",
      "drwxr-xr-x  3 99 99 4.0K Mar  4 15:51 \u001b[0m\u001b[01;34m.\u001b[0m\n",
      "drwxr-xr-x 15 99 99 4.0K Mar  4 15:51 \u001b[01;34m..\u001b[0m\n",
      "-rw-r--r--  1 99 99 6.9K Jul 15  2023 LICENSE\n",
      "-rw-r--r--  1 99 99 4.7K Jul 15  2023 USE_POLICY.md\n",
      "drwxr-xr-x  2 99 99 4.0K Feb 27 23:12 \u001b[01;34mllama-2-7b-chat\u001b[0m\n",
      "-rw-r--r--  1 99 99 489K Jul 13  2023 tokenizer.model\n",
      "-rw-r--r--  1 99 99   50 Jul 13  2023 tokenizer_checklist.chk\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/facebookresearch/llama.git\n",
    "!ls -lah --color llama-models/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cda895d",
   "metadata": {},
   "source": [
    "We also need to clone the huggingface transformers repo, to be able to use the conversion script to convert llama 2 models checkpoint format to huggingface's Transformers format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58aa1f21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'transformers'...\n",
      "remote: Enumerating objects: 187335, done.\u001b[K\n",
      "remote: Counting objects: 100% (759/759), done.\u001b[K\n",
      "remote: Compressing objects: 100% (312/312), done.\u001b[K\n",
      "remote: Total 187335 (delta 470), reused 596 (delta 373), pack-reused 186576\u001b[K\n",
      "Receiving objects: 100% (187335/187335), 207.65 MiB | 40.09 MiB/s, done.\n",
      "Resolving deltas: 100% (131407/131407), done.\n",
      "Updating files: 100% (4096/4096), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1cd72a",
   "metadata": {},
   "source": [
    "Now convert meta checkpoint weights to huggingface format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0dd605a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "Fetching all parameters from the checkpoint at ./llama-models/llama-2-7b-chat.\n",
      "Loading the checkpoint in a Llama model.\n",
      "Loading checkpoint shards: 100%|████████████████| 33/33 [00:06<00:00,  4.90it/s]\n",
      "Saving in the Transformers format.\n"
     ]
    }
   ],
   "source": [
    "!cp llama-models/tokenizer.model llama-models/llama-2-7b-chat/.\n",
    "!python ./transformers/src/transformers/models/llama/convert_llama_weights_to_hf.py --input_dir ./llama-models/llama-2-7b-chat --model_size 7B --output_dir ./hf-weights/7B-chat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e93c14d",
   "metadata": {},
   "source": [
    "## 2. Compile llama 2 models to TensorRT-LLM engines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f098120",
   "metadata": {},
   "source": [
    "Just like TensorRT, TensorRT-LLM provides APIs to compile / convert build LLMs to TensorRT engines. In this example, the conversion steps were implemented already in the `TensorRT-LLM/examples/llama/build.py` script provided by TensorRT-LLM repo. We can analyze the script to see how TensorRT-LLM APIs were used to build the LLM model and load the trained weights.\n",
    "\n",
    "The TensorRT-LLM team is working on high-level APIs to make the conversion steps easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b775cafd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/27/2024-23:16:50] [TRT-LLM] [I] Serially build TensorRT engines.\n",
      "[02/27/2024-23:16:50] [TRT] [I] [MemUsageChange] Init CUDA: CPU +13, GPU +0, now: CPU 118, GPU 417 (MiB)\n",
      "[02/27/2024-23:16:53] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +1974, GPU +350, now: CPU 2228, GPU 767 (MiB)\n",
      "[02/27/2024-23:16:53] [TRT-LLM] [W] Invalid timing cache, using freshly created one\n",
      "[02/27/2024-23:16:58] [TRT-LLM] [I] Loading HF LLaMA ... from ./hf-weights/7B-chat\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:05<00:00,  2.52s/it]\n",
      "[02/27/2024-23:17:23] [TRT-LLM] [I] HF LLaMA loaded. Total time: 00:00:25\n",
      "[02/27/2024-23:17:23] [TRT-LLM] [I] Loading weights from HF LLaMA...\n",
      "[02/27/2024-23:17:59] [TRT-LLM] [I] Weights loaded. Total time: 00:00:35\n",
      "[02/27/2024-23:17:59] [TRT-LLM] [I] Context FMHA Enabled\n",
      "[02/27/2024-23:17:59] [TRT-LLM] [I] Remove Padding Enabled\n",
      "[02/27/2024-23:17:59] [TRT-LLM] [I] Paged KV Cache Enabled\n",
      "[02/27/2024-23:17:59] [TRT-LLM] [I] Build TensorRT engine llama_float16_tp1_rank0.engine\n",
      "[02/27/2024-23:17:59] [TRT] [W] Unused Input: position_ids\n",
      "[02/27/2024-23:17:59] [TRT] [W] [RemoveDeadLayers] Input Tensor position_ids is unused or used only at compile-time, but is not being removed.\n",
      "[02/27/2024-23:17:59] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 28026, GPU 1363 (MiB)\n",
      "[02/27/2024-23:17:59] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +2, GPU +10, now: CPU 28028, GPU 1373 (MiB)\n",
      "[02/27/2024-23:17:59] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.\n",
      "[02/27/2024-23:18:19] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n",
      "[02/27/2024-23:18:19] [TRT] [I] Detected 41 inputs and 1 output network tensors.\n",
      "[02/27/2024-23:18:24] [TRT] [I] Total Host Persistent Memory: 90016\n",
      "[02/27/2024-23:18:24] [TRT] [I] Total Device Persistent Memory: 0\n",
      "[02/27/2024-23:18:24] [TRT] [I] Total Scratch Memory: 33620096\n",
      "[02/27/2024-23:18:24] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 555 steps to complete.\n",
      "[02/27/2024-23:18:24] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 16.7158ms to assign 11 blocks to 555 nodes requiring 1291848704 bytes.\n",
      "[02/27/2024-23:18:24] [TRT] [I] Total Activation Memory: 1291848704\n",
      "[02/27/2024-23:18:24] [TRT] [I] Total Weights Memory: 13476831256\n",
      "[02/27/2024-23:18:24] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 28342, GPU 14241 (MiB)\n",
      "[02/27/2024-23:18:24] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +10, now: CPU 28342, GPU 14251 (MiB)\n",
      "[02/27/2024-23:18:24] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 500 MiB, GPU 66048 MiB\n",
      "[02/27/2024-23:18:24] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +12853, now: CPU 0, GPU 12853 (MiB)\n",
      "[02/27/2024-23:18:30] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 41521 MiB\n",
      "[02/27/2024-23:18:30] [TRT-LLM] [I] Total time of building llama_float16_tp1_rank0.engine: 00:00:31\n",
      "[02/27/2024-23:18:30] [TRT-LLM] [I] Config saved to ./trt-engines/llama_7b/fp16/1-gpu/config.json.\n",
      "[02/27/2024-23:18:30] [TRT-LLM] [I] Serializing engine to ./trt-engines/llama_7b/fp16/1-gpu/llama_float16_tp1_rank0.engine...\n",
      "[02/27/2024-23:18:42] [TRT-LLM] [I] Engine serialized. Total time: 00:00:11\n",
      "[02/27/2024-23:18:42] [TRT-LLM] [I] Timing cache serialized to ./trt-engines/llama_7b/fp16/1-gpu/model.cache\n",
      "[02/27/2024-23:18:42] [TRT-LLM] [I] Total time of building all 1 engines: 00:01:51\n"
     ]
    }
   ],
   "source": [
    "!python TensorRT-LLM/examples/llama/build.py \\\n",
    "    --model_dir ./hf-weights/7B-chat  \\\n",
    "    --dtype float16 \\\n",
    "    --use_gpt_attention_plugin float16  \\\n",
    "    --paged_kv_cache \\\n",
    "    --remove_input_padding \\\n",
    "    --use_gemm_plugin float16  \\\n",
    "    --output_dir \"./trt-engines/llama_7b/fp16/1-gpu\"  \\\n",
    "    --max_input_len 2048 --max_output_len 512 \\\n",
    "    --use_rmsnorm_plugin float16  \\\n",
    "    --enable_context_fmha \\\n",
    "    --use_inflight_batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ca5645",
   "metadata": {},
   "source": [
    "Now let's run the inference of the llama-2-7b-chat model. Similarly, TensorRT-LLM provides APIs to do that. In this example, the inference script `TensorRT-LLM/examples/llama/run.py` is provided by TensorRT-LLM repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47f92980",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the float16 engine ...\n",
      "Input: \"How do I count in French ? 1 un\"\n",
      "Output: \", 2 deux, 3 trois, 4 quatre, 5 cinq, 6 six, 7 sept, 8 huit, 9 neuf, 10 dix.\n",
      "How do you say \"I love you\" in French? Je t'aime.\n",
      "How do you say \"Thank you\" in French? Merci.\n",
      "How do you say \"You're welcome\" in French? De rien.\n",
      "How do you say \"Goodbye\" in\"\n"
     ]
    }
   ],
   "source": [
    "!python TensorRT-LLM/examples/llama/run.py \\\n",
    "    --engine_dir=./trt-engines/llama_7b/fp16/1-gpu \\\n",
    "    --max_output_len 100 \\\n",
    "    --tokenizer_dir \"llama-models\" \\\n",
    "    --input_text \"How do I count in French ? 1 un\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ce322b",
   "metadata": {},
   "source": [
    "## 3. Setup Triton Inference Server for LLM inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69daf0de",
   "metadata": {},
   "source": [
    "To start with Triton, a model repository with certain structure and configuration files should be prepared first. For ease of simplicity, all is setup already in the `triton_model_repo` folder in this example.\n",
    "\n",
    "Here we will setup 2 LLM inference pipeline: the vanilla Pytorch pipeline with optimization, and the optimized TensorRT-LLM pipeline for the llama-2-7b-chat model.\n",
    "- The Python pipeline uses huggingface APIs. The model repo is located at `./triton_model_repo/llama_7b/python/llama-huggingface`\n",
    "- The TensorRT-LLM pipeline contains multiple separated components under `./triton_model_repo/llama_7b/python`: `preprocessing`, `tensorrt_llm` and `postprocessing`. Here we created an `ensemble` folder which encapsulates the `preprocessing`, `postprocessing` and `tensorrt_llm` steps in the same folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08d50370",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]                \u001b[0m\u001b[33m\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]        \u001b[0m\u001b[33m\n",
      "Get:4 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1889 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]      \u001b[0m\n",
      "Get:6 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1517 kB][33m\u001b[33m\n",
      "Get:7 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1074 kB]\n",
      "Get:8 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.6 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB]m\u001b[33m[0m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB]\u001b[0m\u001b[33m\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB][0m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB][0m\u001b[33m\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1346 kB]m\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1927 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1798 kB]\u001b[33m\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [50.4 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [28.1 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [50.4 kB]33m\n",
      "Get:19 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1581 B]3m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
      "Get:20 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [713 kB]\n",
      "Fetched 30.7 MB in 10s (2991 kB/s)                                             \u001b[0m\u001b[33m\u001b[33m\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "83 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  tree\n",
      "0 upgraded, 1 newly installed, 0 to remove and 83 not upgraded.\n",
      "Need to get 47.9 kB of archives.\n",
      "After this operation, 116 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tree amd64 2.0.2-1 [47.9 kB]\n",
      "Fetched 47.9 kB in 0s (567 kB/s)m\u001b[33m\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package tree.\n",
      "(Reading database ... 25642 files and directories currently installed.)\n",
      "Preparing to unpack .../tree_2.0.2-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Unpacking tree (2.0.2-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 40%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Setting up tree (2.0.2-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 60%]\u001b[49m\u001b[39m [##################################........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [##############################################............] \u001b8\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J"
     ]
    }
   ],
   "source": [
    "!apt update && apt install tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43937743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./triton_model_repo/\u001b[0m\n",
      "└── \u001b[01;34mllama_7b\u001b[0m\n",
      "    └── \u001b[01;34mpython\u001b[0m\n",
      "        ├── \u001b[01;34mensemble\u001b[0m\n",
      "        │   ├── \u001b[01;34m1\u001b[0m\n",
      "        │   └── \u001b[01;32mconfig.pbtxt\u001b[0m\n",
      "        ├── \u001b[01;34mllama_huggingface\u001b[0m\n",
      "        │   ├── \u001b[01;34m1\u001b[0m\n",
      "        │   │   ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "        │   │   │   └── \u001b[00mmodel.cpython-310.pyc\u001b[0m\n",
      "        │   │   └── \u001b[00mmodel.py\u001b[0m\n",
      "        │   └── \u001b[00mconfig.pbtxt\u001b[0m\n",
      "        ├── \u001b[01;34mpostprocessing\u001b[0m\n",
      "        │   ├── \u001b[01;34m1\u001b[0m\n",
      "        │   │   ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "        │   │   │   └── \u001b[00mmodel.cpython-310.pyc\u001b[0m\n",
      "        │   │   └── \u001b[00mmodel.py\u001b[0m\n",
      "        │   └── \u001b[01;32mconfig.pbtxt\u001b[0m\n",
      "        ├── \u001b[01;34mpreprocessing\u001b[0m\n",
      "        │   ├── \u001b[01;34m1\u001b[0m\n",
      "        │   │   ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "        │   │   │   └── \u001b[00mmodel.cpython-310.pyc\u001b[0m\n",
      "        │   │   └── \u001b[00mmodel.py\u001b[0m\n",
      "        │   └── \u001b[00mconfig.pbtxt\u001b[0m\n",
      "        └── \u001b[01;34mtensorrt_llm\u001b[0m\n",
      "            ├── \u001b[01;34m1\u001b[0m\n",
      "            └── \u001b[00mconfig.pbtxt\u001b[0m\n",
      "\n",
      "15 directories, 11 files\n"
     ]
    }
   ],
   "source": [
    "!tree ./triton_model_repo/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37aae5c8",
   "metadata": {},
   "source": [
    "Feel free to look at the `config.pbtxt` files in each component folder to understand how Triton configures the inference pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe9471d",
   "metadata": {},
   "source": [
    "Now we can start the Triton server to serve the 2 pipelines\n",
    "\n",
    "Note: \n",
    "- **Start a separate terminal and run the following commands in terminal**\n",
    "- **Make sure that you do not have `.ipynb_checkpoints` under `triton_model_repo/llama_7b/python`, this folder can be auto-generated by jupyter and can mess up the launching of Triton.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "872da69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove ./triton_model_repo/llama_7b/python/.ipynb_checkpoints, which can mess up launch of triton\n",
    "!rm -rf ./triton_model_repo/llama_7b/python/.ipynb_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60e7e188",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0304 16:08:03.536612 6090 cache_manager.cc:480] Create CacheManager with cache_dir: '/opt/tritonserver/caches'\n",
      "I0304 16:08:03.800840 6090 pinned_memory_manager.cc:241] Pinned memory pool is created at '0x7f5dac000000' with size 268435456\n",
      "I0304 16:08:03.801513 6090 cuda_memory_manager.cc:107] CUDA memory pool is created on device 0 with size 67108864\n",
      "I0304 16:08:03.807170 6090 model_config_utils.cc:680] Server side auto-completed config: name: \"ensemble\"\n",
      "platform: \"ensemble\"\n",
      "max_batch_size: 128\n",
      "input {\n",
      "  name: \"text_input\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: -1\n",
      "}\n",
      "input {\n",
      "  name: \"max_tokens\"\n",
      "  data_type: TYPE_UINT32\n",
      "  dims: -1\n",
      "}\n",
      "input {\n",
      "  name: \"bad_words\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: -1\n",
      "}\n",
      "input {\n",
      "  name: \"stop_words\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: -1\n",
      "}\n",
      "input {\n",
      "  name: \"end_id\"\n",
      "  data_type: TYPE_UINT32\n",
      "  dims: 1\n",
      "  optional: true\n",
      "}\n",
      "input {\n",
      "  name: \"pad_id\"\n",
      "  data_type: TYPE_UINT32\n",
      "  dims: 1\n",
      "  optional: true\n",
      "}\n",
      "input {\n",
      "  name: \"top_k\"\n",
      "  data_type: TYPE_UINT32\n",
      "  dims: 1\n",
      "  optional: true\n",
      "}\n",
      "input {\n",
      "  name: \"top_p\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: 1\n",
      "  optional: true\n",
      "}\n",
      "input {\n",
      "  name: \"temperature\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: 1\n",
      "  optional: true\n",
      "}\n",
      "input {\n",
      "  name: \"length_penalty\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: 1\n",
      "  optional: true\n",
      "}\n",
      "input {\n",
      "  name: \"repetition_penalty\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: 1\n",
      "  optional: true\n",
      "}\n",
      "input {\n",
      "  name: \"min_length\"\n",
      "  data_type: TYPE_UINT32\n",
      "  dims: 1\n",
      "  optional: true\n",
      "}\n",
      "input {\n",
      "  name: \"presence_penalty\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: 1\n",
      "  optional: true\n",
      "}\n",
      "input {\n",
      "  name: \"random_seed\"\n",
      "  data_type: TYPE_UINT64\n",
      "  dims: 1\n",
      "  optional: true\n",
      "}\n",
      "input {\n",
      "  name: \"beam_width\"\n",
      "  data_type: TYPE_UINT32\n",
      "  dims: 1\n",
      "  optional: true\n",
      "}\n",
      "input {\n",
      "  name: \"stream\"\n",
      "  data_type: TYPE_BOOL\n",
      "  dims: 1\n",
      "  optional: true\n",
      "}\n",
      "output {\n",
      "  name: \"text_output\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: -1\n",
      "  dims: -1\n",
      "}\n",
      "ensemble_scheduling {\n",
      "  step {\n",
      "    model_name: \"preprocessing\"\n",
      "    model_version: -1\n",
      "    input_map {\n",
      "      key: \"BAD_WORDS_DICT\"\n",
      "      value: \"bad_words\"\n",
      "    }\n",
      "    input_map {\n",
      "      key: \"QUERY\"\n",
      "      value: \"text_input\"\n",
      "    }\n",
      "    input_map {\n",
      "      key: \"REQUEST_OUTPUT_LEN\"\n",
      "      value: \"max_tokens\"\n",
      "    }\n",
      "    input_map {\n",
      "      key: \"STOP_WORDS_DICT\"\n",
      "      value: \"stop_words\"\n",
      "    }\n",
      "    output_map {\n",
      "      key: \"INPUT_ID\"\n",
      "      value: \"_INPUT_ID\"\n",
      "    }\n",
      "    output_map {\n",
      "      key: \"REQUEST_INPUT_LEN\"\n",
      "      value: \"_REQUEST_INPUT_LEN\"\n",
      "    }\n",
      "    output_map {\n",
      "      key: \"REQUEST_OUTPUT_LEN\"\n",
      "      value: \"_REQUEST_OUTPUT_LEN\"\n",
      "    }\n",
      "  }\n",
      "  step {\n",
      "    model_name: \"tensorrt_llm\"\n",
      "    model_version: -1\n",
      "    input_map {\n",
      "      key: \"beam_width\"\n",
      "      value: \"beam_width\"\n",
      "    }\n",
      "    input_map {\n",
      "      key: \"end_id\"\n",
      "      value: \"end_id\"\n",
      "    }\n",
      "    input_map {\n",
      "      key: \"input_ids\"\n",
      "      value: \"_INPUT_ID\"\n",
      "    }\n",
      "    input_map {\n",
      "      key: \"input_lengths\"\n",
      "      value: \"_REQUEST_INPUT_LEN\"\n",
      "    }\n",
      "    input_map {\n",
      "      key: \"len_penalty\"\n",
      "      value: \"length_penalty\"\n",
      "    }\n",
      "    input_map {\n",
      "      key: \"min_length\"\n",
      "      value: \"min_length\"\n",
      "    }\n",
      "    input_map {\n",
      "      key: \"pad_id\"\n",
      "      value: \"pad_id\"\n",
      "    }\n",
      "    input_map {\n",
      "      key: \"presence_penalty\"\n",
      "      value: \"presence_penalty\"\n",
      "    }\n",
      "    input_map {\n",
      "      key: \"random_seed\"\n",
      "      value: \"random_seed\"\n",
      "    }\n",
      "    input_map {\n",
      "      key: \"repetition_penalty\"\n",
      "      value: \"repetition_penalty\"\n",
      "    }\n",
      "    input_map {\n",
      "      key: \"request_output_len\"\n",
      "      value: \"_REQUEST_OUTPUT_LEN\"\n",
      "    }\n",
      "    input_map {\n",
      "      key: \"runtime_top_k\"\n",
      "      value: \"top_k\"\n",
      "    }\n",
      "    input_map {\n",
      "      key: \"runtime_top_p\"\n",
      "      value: \"top_p\"\n",
      "    }\n",
      "    input_map {\n",
      "      key: \"streaming\"\n",
      "      value: \"stream\"\n",
      "    }\n",
      "    input_map {\n",
      "      key: \"temperature\"\n",
      "      value: \"temperature\"\n",
      "    }\n",
      "    output_map {\n",
      "      key: \"output_ids\"\n",
      "      value: \"_TOKENS_BATCH\"\n",
      "    }\n",
      "  }\n",
      "  step {\n",
      "    model_name: \"postprocessing\"\n",
      "    model_version: -1\n",
      "    input_map {\n",
      "      key: \"TOKENS_BATCH\"\n",
      "      value: \"_TOKENS_BATCH\"\n",
      "    }\n",
      "    output_map {\n",
      "      key: \"OUTPUT\"\n",
      "      value: \"text_output\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "I0304 16:08:03.811287 6090 model_config_utils.cc:680] Server side auto-completed config: name: \"llama_huggingface\"\n",
      "input {\n",
      "  name: \"text_input\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: -1\n",
      "}\n",
      "input {\n",
      "  name: \"max_tokens\"\n",
      "  data_type: TYPE_UINT32\n",
      "  dims: -1\n",
      "}\n",
      "output {\n",
      "  name: \"text_output\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: -1\n",
      "}\n",
      "instance_group {\n",
      "  count: 1\n",
      "  gpus: 0\n",
      "  kind: KIND_GPU\n",
      "}\n",
      "default_model_filename: \"model.py\"\n",
      "parameters {\n",
      "  key: \"huggingface_model\"\n",
      "  value {\n",
      "    string_value: \"/workspace/notebooks/tensorrt-llm/hf-weights/7B-chat\"\n",
      "  }\n",
      "}\n",
      "backend: \"python\"\n",
      "\n",
      "I0304 16:08:03.815230 6090 model_config_utils.cc:680] Server side auto-completed config: name: \"postprocessing\"\n",
      "max_batch_size: 128\n",
      "input {\n",
      "  name: \"TOKENS_BATCH\"\n",
      "  data_type: TYPE_INT32\n",
      "  dims: -1\n",
      "  dims: -1\n",
      "}\n",
      "output {\n",
      "  name: \"OUTPUT\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: -1\n",
      "  dims: -1\n",
      "}\n",
      "instance_group {\n",
      "  count: 1\n",
      "  kind: KIND_CPU\n",
      "}\n",
      "default_model_filename: \"model.py\"\n",
      "parameters {\n",
      "  key: \"tokenizer_dir\"\n",
      "  value {\n",
      "    string_value: \"/workspace/notebooks/tensorrt-llm/llama-models\"\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  key: \"tokenizer_type\"\n",
      "  value {\n",
      "    string_value: \"llama\"\n",
      "  }\n",
      "}\n",
      "backend: \"python\"\n",
      "\n",
      "I0304 16:08:03.819067 6090 model_config_utils.cc:680] Server side auto-completed config: name: \"preprocessing\"\n",
      "max_batch_size: 128\n",
      "input {\n",
      "  name: \"QUERY\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: -1\n",
      "}\n",
      "input {\n",
      "  name: \"BAD_WORDS_DICT\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: -1\n",
      "}\n",
      "input {\n",
      "  name: \"STOP_WORDS_DICT\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: -1\n",
      "}\n",
      "input {\n",
      "  name: \"REQUEST_OUTPUT_LEN\"\n",
      "  data_type: TYPE_UINT32\n",
      "  dims: -1\n",
      "}\n",
      "output {\n",
      "  name: \"INPUT_ID\"\n",
      "  data_type: TYPE_INT32\n",
      "  dims: -1\n",
      "}\n",
      "output {\n",
      "  name: \"REQUEST_INPUT_LEN\"\n",
      "  data_type: TYPE_INT32\n",
      "  dims: 1\n",
      "}\n",
      "output {\n",
      "  name: \"BAD_WORDS_IDS\"\n",
      "  data_type: TYPE_INT32\n",
      "  dims: 2\n",
      "  dims: -1\n",
      "}\n",
      "output {\n",
      "  name: \"STOP_WORDS_IDS\"\n",
      "  data_type: TYPE_INT32\n",
      "  dims: 2\n",
      "  dims: -1\n",
      "}\n",
      "output {\n",
      "  name: \"REQUEST_OUTPUT_LEN\"\n",
      "  data_type: TYPE_UINT32\n",
      "  dims: -1\n",
      "}\n",
      "instance_group {\n",
      "  count: 1\n",
      "  kind: KIND_CPU\n",
      "}\n",
      "default_model_filename: \"model.py\"\n",
      "parameters {\n",
      "  key: \"tokenizer_dir\"\n",
      "  value {\n",
      "    string_value: \"/workspace/notebooks/tensorrt-llm/llama-models\"\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  key: \"tokenizer_type\"\n",
      "  value {\n",
      "    string_value: \"llama\"\n",
      "  }\n",
      "}\n",
      "backend: \"python\"\n",
      "\n",
      "I0304 16:08:03.822015 6090 model_config_utils.cc:680] Server side auto-completed config: name: \"tensorrt_llm\"\n",
      "max_batch_size: 128\n",
      "input {\n",
      "  name: \"input_ids\"\n",
      "  data_type: TYPE_INT32\n",
      "  dims: -1\n",
      "}\n",
      "input {\n",
      "  name: \"input_lengths\"\n",
      "  data_type: TYPE_INT32\n",
      "  dims: 1\n",
      "  reshape {\n",
      "  }\n",
      "}\n",
      "input {\n",
      "  name: \"request_output_len\"\n",
      "  data_type: TYPE_UINT32\n",
      "  dims: 1\n",
      "}\n",
      "input {\n",
      "  name: \"end_id\"\n",
      "  data_type: TYPE_UINT32\n",
      "  dims: 1\n",
      "  reshape {\n",
      "  }\n",
      "  optional: true\n",
      "}\n",
      "input {\n",
      "  name: \"pad_id\"\n",
      "  data_type: TYPE_UINT32\n",
      "  dims: 1\n",
      "  reshape {\n",
      "  }\n",
      "  optional: true\n",
      "}\n",
      "input {\n",
      "  name: \"beam_width\"\n",
      "  data_type: TYPE_UINT32\n",
      "  dims: 1\n",
      "  reshape {\n",
      "  }\n",
      "  optional: true\n",
      "}\n",
      "input {\n",
      "  name: \"temperature\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: 1\n",
      "  reshape {\n",
      "  }\n",
      "  optional: true\n",
      "}\n",
      "input {\n",
      "  name: \"runtime_top_k\"\n",
      "  data_type: TYPE_UINT32\n",
      "  dims: 1\n",
      "  reshape {\n",
      "  }\n",
      "  optional: true\n",
      "}\n",
      "input {\n",
      "  name: \"runtime_top_p\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: 1\n",
      "  reshape {\n",
      "  }\n",
      "  optional: true\n",
      "}\n",
      "input {\n",
      "  name: \"len_penalty\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: 1\n",
      "  reshape {\n",
      "  }\n",
      "  optional: true\n",
      "}\n",
      "input {\n",
      "  name: \"repetition_penalty\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: 1\n",
      "  reshape {\n",
      "  }\n",
      "  optional: true\n",
      "}\n",
      "input {\n",
      "  name: \"min_length\"\n",
      "  data_type: TYPE_UINT32\n",
      "  dims: 1\n",
      "  reshape {\n",
      "  }\n",
      "  optional: true\n",
      "}\n",
      "input {\n",
      "  name: \"presence_penalty\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: 1\n",
      "  reshape {\n",
      "  }\n",
      "  optional: true\n",
      "}\n",
      "input {\n",
      "  name: \"random_seed\"\n",
      "  data_type: TYPE_UINT64\n",
      "  dims: 1\n",
      "  reshape {\n",
      "  }\n",
      "  optional: true\n",
      "}\n",
      "input {\n",
      "  name: \"stop\"\n",
      "  data_type: TYPE_BOOL\n",
      "  dims: 1\n",
      "  optional: true\n",
      "}\n",
      "input {\n",
      "  name: \"streaming\"\n",
      "  data_type: TYPE_BOOL\n",
      "  dims: 1\n",
      "  optional: true\n",
      "}\n",
      "output {\n",
      "  name: \"output_ids\"\n",
      "  data_type: TYPE_INT32\n",
      "  dims: -1\n",
      "  dims: -1\n",
      "}\n",
      "instance_group {\n",
      "  count: 1\n",
      "  kind: KIND_CPU\n",
      "}\n",
      "parameters {\n",
      "  key: \"FORCE_CPU_ONLY_INPUT_TENSORS\"\n",
      "  value {\n",
      "    string_value: \"no\"\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  key: \"batch_scheduler_policy\"\n",
      "  value {\n",
      "    string_value: \"guaranteed_completion\"\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  key: \"enable_trt_overlap\"\n",
      "  value {\n",
      "    string_value: \"${enable_trt_overlap}\"\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  key: \"gpt_model_path\"\n",
      "  value {\n",
      "    string_value: \"/workspace/notebooks/tensorrt-llm/trt-engines/llama_7b/fp16/1-gpu\"\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  key: \"gpt_model_type\"\n",
      "  value {\n",
      "    string_value: \"V1\"\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  key: \"kv_cache_free_gpu_mem_fraction\"\n",
      "  value {\n",
      "    string_value: \"${kv_cache_free_gpu_mem_fraction}\"\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  key: \"max_beam_width\"\n",
      "  value {\n",
      "    string_value: \"1\"\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  key: \"max_num_sequences\"\n",
      "  value {\n",
      "    string_value: \"${max_num_sequences}\"\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  key: \"max_tokens_in_paged_kv_cache\"\n",
      "  value {\n",
      "  }\n",
      "}\n",
      "backend: \"tensorrtllm\"\n",
      "model_transaction_policy {\n",
      "}\n",
      "\n",
      "I0304 16:08:03.822181 6090 model_lifecycle.cc:430] AsyncLoad() 'tensorrt_llm'\n",
      "W0304 16:08:03.822763 6090 model_lifecycle.cc:108] ignore version directory '.ipynb_checkpoints' which fails to convert to integral number\n",
      "I0304 16:08:03.822791 6090 model_lifecycle.cc:461] loading: tensorrt_llm:1\n",
      "I0304 16:08:03.822800 6090 model_lifecycle.cc:430] AsyncLoad() 'llama_huggingface'\n",
      "I0304 16:08:03.822831 6090 model_lifecycle.cc:531] CreateModel() 'tensorrt_llm' version 1\n",
      "I0304 16:08:03.823016 6090 backend_model.cc:445] Adding default backend config setting: default-max-batch-size,4\n",
      "I0304 16:08:03.823029 6090 shared_library.cc:112] OpenLibraryHandle: /opt/tritonserver/backends/tensorrtllm/libtriton_tensorrtllm.so\n",
      "W0304 16:08:03.823280 6090 model_lifecycle.cc:108] ignore version directory '.ipynb_checkpoints' which fails to convert to integral number\n",
      "I0304 16:08:03.823292 6090 model_lifecycle.cc:461] loading: llama_huggingface:1\n",
      "I0304 16:08:03.823299 6090 model_lifecycle.cc:430] AsyncLoad() 'postprocessing'\n",
      "I0304 16:08:03.823324 6090 model_lifecycle.cc:531] CreateModel() 'llama_huggingface' version 1\n",
      "I0304 16:08:03.823386 6090 backend_model.cc:445] Adding default backend config setting: default-max-batch-size,4\n",
      "W0304 16:08:03.823782 6090 model_lifecycle.cc:108] ignore version directory '.ipynb_checkpoints' which fails to convert to integral number\n",
      "I0304 16:08:03.823794 6090 model_lifecycle.cc:461] loading: postprocessing:1\n",
      "I0304 16:08:03.823801 6090 model_lifecycle.cc:430] AsyncLoad() 'preprocessing'\n",
      "I0304 16:08:03.823825 6090 model_lifecycle.cc:531] CreateModel() 'postprocessing' version 1\n",
      "I0304 16:08:03.823875 6090 backend_model.cc:445] Adding default backend config setting: default-max-batch-size,4\n",
      "W0304 16:08:03.824276 6090 model_lifecycle.cc:108] ignore version directory '.ipynb_checkpoints' which fails to convert to integral number\n",
      "I0304 16:08:03.824291 6090 model_lifecycle.cc:461] loading: preprocessing:1\n",
      "I0304 16:08:03.824319 6090 model_lifecycle.cc:531] CreateModel() 'preprocessing' version 1\n",
      "I0304 16:08:03.824369 6090 backend_model.cc:445] Adding default backend config setting: default-max-batch-size,4\n",
      "I0304 16:08:03.856631 6090 shared_library.cc:112] OpenLibraryHandle: /opt/tritonserver/backends/python/libtriton_python.so\n",
      "I0304 16:08:03.857733 6090 model_config_utils.cc:1881] ModelConfig 64-bit fields:\n",
      "I0304 16:08:03.857746 6090 model_config_utils.cc:1883] \tModelConfig::dynamic_batching::default_priority_level\n",
      "I0304 16:08:03.857751 6090 model_config_utils.cc:1883] \tModelConfig::dynamic_batching::default_queue_policy::default_timeout_microseconds\n",
      "I0304 16:08:03.857755 6090 model_config_utils.cc:1883] \tModelConfig::dynamic_batching::max_queue_delay_microseconds\n",
      "I0304 16:08:03.857759 6090 model_config_utils.cc:1883] \tModelConfig::dynamic_batching::priority_levels\n",
      "I0304 16:08:03.857765 6090 model_config_utils.cc:1883] \tModelConfig::dynamic_batching::priority_queue_policy::key\n",
      "I0304 16:08:03.857773 6090 model_config_utils.cc:1883] \tModelConfig::dynamic_batching::priority_queue_policy::value::default_timeout_microseconds\n",
      "I0304 16:08:03.857778 6090 model_config_utils.cc:1883] \tModelConfig::ensemble_scheduling::step::model_version\n",
      "I0304 16:08:03.857782 6090 model_config_utils.cc:1883] \tModelConfig::input::dims\n",
      "I0304 16:08:03.857786 6090 model_config_utils.cc:1883] \tModelConfig::input::reshape::shape\n",
      "I0304 16:08:03.857790 6090 model_config_utils.cc:1883] \tModelConfig::instance_group::secondary_devices::device_id\n",
      "I0304 16:08:03.857794 6090 model_config_utils.cc:1883] \tModelConfig::model_warmup::inputs::value::dims\n",
      "I0304 16:08:03.857800 6090 model_config_utils.cc:1883] \tModelConfig::optimization::cuda::graph_spec::graph_lower_bound::input::value::dim\n",
      "I0304 16:08:03.857803 6090 model_config_utils.cc:1883] \tModelConfig::optimization::cuda::graph_spec::input::value::dim\n",
      "I0304 16:08:03.857809 6090 model_config_utils.cc:1883] \tModelConfig::output::dims\n",
      "I0304 16:08:03.857813 6090 model_config_utils.cc:1883] \tModelConfig::output::reshape::shape\n",
      "I0304 16:08:03.857820 6090 model_config_utils.cc:1883] \tModelConfig::sequence_batching::direct::max_queue_delay_microseconds\n",
      "I0304 16:08:03.857824 6090 model_config_utils.cc:1883] \tModelConfig::sequence_batching::max_sequence_idle_microseconds\n",
      "I0304 16:08:03.857827 6090 model_config_utils.cc:1883] \tModelConfig::sequence_batching::oldest::max_queue_delay_microseconds\n",
      "I0304 16:08:03.857830 6090 model_config_utils.cc:1883] \tModelConfig::sequence_batching::state::dims\n",
      "I0304 16:08:03.857834 6090 model_config_utils.cc:1883] \tModelConfig::sequence_batching::state::initial_state::dims\n",
      "I0304 16:08:03.857838 6090 model_config_utils.cc:1883] \tModelConfig::version_policy::specific::versions\n",
      "I0304 16:08:03.858228 6090 python_be.cc:1903] 'python' TRITONBACKEND API version: 1.16\n",
      "I0304 16:08:03.858237 6090 python_be.cc:1925] backend configuration:\n",
      "{\"cmdline\":{\"auto-complete-config\":\"true\",\"backend-directory\":\"/opt/tritonserver/backends\",\"min-compute-capability\":\"6.000000\",\"default-max-batch-size\":\"4\"}}\n",
      "I0304 16:08:03.858254 6090 python_be.cc:2064] Shared memory configuration is shm-default-byte-size=1048576,shm-growth-byte-size=1048576,stub-timeout-seconds=30\n",
      "I0304 16:08:03.858721 6090 python_be.cc:2353] TRITONBACKEND_GetBackendAttribute: setting attributes\n",
      "[TensorRT-LLM][WARNING] max_tokens_in_paged_kv_cache is not specified, will use default value\n",
      "[TensorRT-LLM][WARNING] batch_scheduler_policy parameter was not found or is invalid (must be max_utilization or guaranteed_no_evict)\n",
      "[TensorRT-LLM][WARNING] kv_cache_free_gpu_mem_fraction is not specified, will use default value of 0.85 or max_tokens_in_paged_kv_cache\n",
      "[TensorRT-LLM][WARNING] max_num_sequences is not specified, will be set to the TRT engine max_batch_size\n",
      "[TensorRT-LLM][WARNING] enable_trt_overlap is not specified, will be set to true\n",
      "[TensorRT-LLM][WARNING] [json.exception.type_error.302] type must be number, but is null\n",
      "[TensorRT-LLM][WARNING] Optional value for parameter max_num_tokens will not be set.\n",
      "[TensorRT-LLM][INFO] Initializing MPI with thread mode 1\n",
      "I0304 16:08:03.860741 6090 python_be.cc:2155] TRITONBACKEND_ModelInitialize: llama_huggingface (version 1)\n",
      "I0304 16:08:03.860770 6090 python_be.cc:2155] TRITONBACKEND_ModelInitialize: postprocessing (version 1)\n",
      "I0304 16:08:03.860802 6090 python_be.cc:2155] TRITONBACKEND_ModelInitialize: preprocessing (version 1)\n",
      "I0304 16:08:03.861442 6090 stub_launcher.cc:254] Starting Python backend stub:  exec /opt/tritonserver/backends/python/triton_python_backend_stub /workspace/notebooks/tensorrt-llm/triton_model_repo/llama_7b/python/preprocessing/1/model.py triton_python_backend_shm_region_3 1048576 1048576 6090 /opt/tritonserver/backends/python 336 preprocessing DEFAULT\n",
      "I0304 16:08:03.861444 6090 stub_launcher.cc:254] Starting Python backend stub:  exec /opt/tritonserver/backends/python/triton_python_backend_stub /workspace/notebooks/tensorrt-llm/triton_model_repo/llama_7b/python/postprocessing/1/model.py triton_python_backend_shm_region_1 1048576 1048576 6090 /opt/tritonserver/backends/python 336 postprocessing DEFAULT\n",
      "I0304 16:08:03.861445 6090 stub_launcher.cc:254] Starting Python backend stub:  exec /opt/tritonserver/backends/python/triton_python_backend_stub /workspace/notebooks/tensorrt-llm/triton_model_repo/llama_7b/python/llama_huggingface/1/model.py triton_python_backend_shm_region_2 1048576 1048576 6090 /opt/tritonserver/backends/python 336 llama_huggingface DEFAULT\n",
      "[TensorRT-LLM][INFO] MPI size: 1, rank: 0\n",
      "I0304 16:08:05.765242 6090 python_be.cc:1882] model configuration:\n",
      "{\n",
      "    \"name\": \"postprocessing\",\n",
      "    \"platform\": \"\",\n",
      "    \"backend\": \"python\",\n",
      "    \"version_policy\": {\n",
      "        \"latest\": {\n",
      "            \"num_versions\": 1\n",
      "        }\n",
      "    },\n",
      "    \"max_batch_size\": 128,\n",
      "    \"input\": [\n",
      "        {\n",
      "            \"name\": \"TOKENS_BATCH\",\n",
      "            \"data_type\": \"TYPE_INT32\",\n",
      "            \"format\": \"FORMAT_NONE\",\n",
      "            \"dims\": [\n",
      "                -1,\n",
      "                -1\n",
      "            ],\n",
      "            \"is_shape_tensor\": false,\n",
      "            \"allow_ragged_batch\": false,\n",
      "            \"optional\": false\n",
      "        }\n",
      "    ],\n",
      "    \"output\": [\n",
      "        {\n",
      "            \"name\": \"OUTPUT\",\n",
      "            \"data_type\": \"TYPE_STRING\",\n",
      "            \"dims\": [\n",
      "                -1,\n",
      "                -1\n",
      "            ],\n",
      "            \"label_filename\": \"\",\n",
      "            \"is_shape_tensor\": false\n",
      "        }\n",
      "    ],\n",
      "    \"batch_input\": [],\n",
      "    \"batch_output\": [],\n",
      "    \"optimization\": {\n",
      "        \"priority\": \"PRIORITY_DEFAULT\",\n",
      "        \"input_pinned_memory\": {\n",
      "            \"enable\": true\n",
      "        },\n",
      "        \"output_pinned_memory\": {\n",
      "            \"enable\": true\n",
      "        },\n",
      "        \"gather_kernel_buffer_threshold\": 0,\n",
      "        \"eager_batching\": false\n",
      "    },\n",
      "    \"instance_group\": [\n",
      "        {\n",
      "            \"name\": \"postprocessing_0\",\n",
      "            \"kind\": \"KIND_CPU\",\n",
      "            \"count\": 1,\n",
      "            \"gpus\": [],\n",
      "            \"secondary_devices\": [],\n",
      "            \"profile\": [],\n",
      "            \"passive\": false,\n",
      "            \"host_policy\": \"\"\n",
      "        }\n",
      "    ],\n",
      "    \"default_model_filename\": \"model.py\",\n",
      "    \"cc_model_filenames\": {},\n",
      "    \"metric_tags\": {},\n",
      "    \"parameters\": {\n",
      "        \"tokenizer_type\": {\n",
      "            \"string_value\": \"llama\"\n",
      "        },\n",
      "        \"tokenizer_dir\": {\n",
      "            \"string_value\": \"/workspace/notebooks/tensorrt-llm/llama-models\"\n",
      "        }\n",
      "    },\n",
      "    \"model_warmup\": []\n",
      "}\n",
      "I0304 16:08:05.765599 6090 python_be.cc:2199] TRITONBACKEND_ModelInstanceInitialize: postprocessing_0_0 (CPU device 0)\n",
      "I0304 16:08:05.765633 6090 backend_model_instance.cc:69] Creating instance postprocessing_0_0 on CPU using artifact 'model.py'\n",
      "I0304 16:08:05.766177 6090 stub_launcher.cc:254] Starting Python backend stub:  exec /opt/tritonserver/backends/python/triton_python_backend_stub /workspace/notebooks/tensorrt-llm/triton_model_repo/llama_7b/python/postprocessing/1/model.py triton_python_backend_shm_region_4 1048576 1048576 6090 /opt/tritonserver/backends/python 336 postprocessing_0_0 DEFAULT\n",
      "I0304 16:08:06.443474 6090 python_be.cc:2220] TRITONBACKEND_ModelInstanceInitialize: instance initialization successful postprocessing_0_0 (device 0)\n",
      "I0304 16:08:06.443582 6090 backend_model_instance.cc:772] Starting backend thread for postprocessing_0_0 at nice 0 on device 0...\n",
      "I0304 16:08:06.443623 6090 backend_model.cc:617] Created model instance named 'postprocessing_0_0' with device id '0'\n",
      "I0304 16:08:06.443768 6090 model_lifecycle.cc:675] OnLoadComplete() 'postprocessing' version 1\n",
      "I0304 16:08:06.443802 6090 model_lifecycle.cc:713] OnLoadFinal() 'postprocessing' for all version(s)\n",
      "I0304 16:08:06.443810 6090 model_lifecycle.cc:818] successfully loaded 'postprocessing'\n",
      "I0304 16:08:06.897604 6090 python_be.cc:1882] model configuration:\n",
      "{\n",
      "    \"name\": \"llama_huggingface\",\n",
      "    \"platform\": \"\",\n",
      "    \"backend\": \"python\",\n",
      "    \"version_policy\": {\n",
      "        \"latest\": {\n",
      "            \"num_versions\": 1\n",
      "        }\n",
      "    },\n",
      "    \"max_batch_size\": 0,\n",
      "    \"input\": [\n",
      "        {\n",
      "            \"name\": \"text_input\",\n",
      "            \"data_type\": \"TYPE_STRING\",\n",
      "            \"format\": \"FORMAT_NONE\",\n",
      "            \"dims\": [\n",
      "                -1\n",
      "            ],\n",
      "            \"is_shape_tensor\": false,\n",
      "            \"allow_ragged_batch\": false,\n",
      "            \"optional\": false\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"max_tokens\",\n",
      "            \"data_type\": \"TYPE_UINT32\",\n",
      "            \"format\": \"FORMAT_NONE\",\n",
      "            \"dims\": [\n",
      "                -1\n",
      "            ],\n",
      "            \"is_shape_tensor\": false,\n",
      "            \"allow_ragged_batch\": false,\n",
      "            \"optional\": false\n",
      "        }\n",
      "    ],\n",
      "    \"output\": [\n",
      "        {\n",
      "            \"name\": \"text_output\",\n",
      "            \"data_type\": \"TYPE_STRING\",\n",
      "            \"dims\": [\n",
      "                -1\n",
      "            ],\n",
      "            \"label_filename\": \"\",\n",
      "            \"is_shape_tensor\": false\n",
      "        }\n",
      "    ],\n",
      "    \"batch_input\": [],\n",
      "    \"batch_output\": [],\n",
      "    \"optimization\": {\n",
      "        \"priority\": \"PRIORITY_DEFAULT\",\n",
      "        \"input_pinned_memory\": {\n",
      "            \"enable\": true\n",
      "        },\n",
      "        \"output_pinned_memory\": {\n",
      "            \"enable\": true\n",
      "        },\n",
      "        \"gather_kernel_buffer_threshold\": 0,\n",
      "        \"eager_batching\": false\n",
      "    },\n",
      "    \"instance_group\": [\n",
      "        {\n",
      "            \"name\": \"llama_huggingface_0\",\n",
      "            \"kind\": \"KIND_GPU\",\n",
      "            \"count\": 1,\n",
      "            \"gpus\": [\n",
      "                0\n",
      "            ],\n",
      "            \"secondary_devices\": [],\n",
      "            \"profile\": [],\n",
      "            \"passive\": false,\n",
      "            \"host_policy\": \"\"\n",
      "        }\n",
      "    ],\n",
      "    \"default_model_filename\": \"model.py\",\n",
      "    \"cc_model_filenames\": {},\n",
      "    \"metric_tags\": {},\n",
      "    \"parameters\": {\n",
      "        \"huggingface_model\": {\n",
      "            \"string_value\": \"/workspace/notebooks/tensorrt-llm/hf-weights/7B-chat\"\n",
      "        }\n",
      "    },\n",
      "    \"model_warmup\": []\n",
      "}\n",
      "I0304 16:08:06.897952 6090 python_be.cc:2199] TRITONBACKEND_ModelInstanceInitialize: llama_huggingface_0_0 (GPU device 0)\n",
      "I0304 16:08:06.898400 6090 backend_model_instance.cc:106] Creating instance llama_huggingface_0_0 on GPU 0 (8.0) using artifact 'model.py'\n",
      "I0304 16:08:06.898917 6090 stub_launcher.cc:254] Starting Python backend stub:  exec /opt/tritonserver/backends/python/triton_python_backend_stub /workspace/notebooks/tensorrt-llm/triton_model_repo/llama_7b/python/llama_huggingface/1/model.py triton_python_backend_shm_region_5 1048576 1048576 6090 /opt/tritonserver/backends/python 336 llama_huggingface_0_0 DEFAULT\n",
      "I0304 16:08:07.200996 6090 python_be.cc:1882] model configuration:\n",
      "{\n",
      "    \"name\": \"preprocessing\",\n",
      "    \"platform\": \"\",\n",
      "    \"backend\": \"python\",\n",
      "    \"version_policy\": {\n",
      "        \"latest\": {\n",
      "            \"num_versions\": 1\n",
      "        }\n",
      "    },\n",
      "    \"max_batch_size\": 128,\n",
      "    \"input\": [\n",
      "        {\n",
      "            \"name\": \"QUERY\",\n",
      "            \"data_type\": \"TYPE_STRING\",\n",
      "            \"format\": \"FORMAT_NONE\",\n",
      "            \"dims\": [\n",
      "                -1\n",
      "            ],\n",
      "            \"is_shape_tensor\": false,\n",
      "            \"allow_ragged_batch\": false,\n",
      "            \"optional\": false\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"BAD_WORDS_DICT\",\n",
      "            \"data_type\": \"TYPE_STRING\",\n",
      "            \"format\": \"FORMAT_NONE\",\n",
      "            \"dims\": [\n",
      "                -1\n",
      "            ],\n",
      "            \"is_shape_tensor\": false,\n",
      "            \"allow_ragged_batch\": false,\n",
      "            \"optional\": false\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"STOP_WORDS_DICT\",\n",
      "            \"data_type\": \"TYPE_STRING\",\n",
      "            \"format\": \"FORMAT_NONE\",\n",
      "            \"dims\": [\n",
      "                -1\n",
      "            ],\n",
      "            \"is_shape_tensor\": false,\n",
      "            \"allow_ragged_batch\": false,\n",
      "            \"optional\": false\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"REQUEST_OUTPUT_LEN\",\n",
      "            \"data_type\": \"TYPE_UINT32\",\n",
      "            \"format\": \"FORMAT_NONE\",\n",
      "            \"dims\": [\n",
      "                -1\n",
      "            ],\n",
      "            \"is_shape_tensor\": false,\n",
      "            \"allow_ragged_batch\": false,\n",
      "            \"optional\": false\n",
      "        }\n",
      "    ],\n",
      "    \"output\": [\n",
      "        {\n",
      "            \"name\": \"INPUT_ID\",\n",
      "            \"data_type\": \"TYPE_INT32\",\n",
      "            \"dims\": [\n",
      "                -1\n",
      "            ],\n",
      "            \"label_filename\": \"\",\n",
      "            \"is_shape_tensor\": false\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"REQUEST_INPUT_LEN\",\n",
      "            \"data_type\": \"TYPE_INT32\",\n",
      "            \"dims\": [\n",
      "                1\n",
      "            ],\n",
      "            \"label_filename\": \"\",\n",
      "            \"is_shape_tensor\": false\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"BAD_WORDS_IDS\",\n",
      "            \"data_type\": \"TYPE_INT32\",\n",
      "            \"dims\": [\n",
      "                2,\n",
      "                -1\n",
      "            ],\n",
      "            \"label_filename\": \"\",\n",
      "            \"is_shape_tensor\": false\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"STOP_WORDS_IDS\",\n",
      "            \"data_type\": \"TYPE_INT32\",\n",
      "            \"dims\": [\n",
      "                2,\n",
      "                -1\n",
      "            ],\n",
      "            \"label_filename\": \"\",\n",
      "            \"is_shape_tensor\": false\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"REQUEST_OUTPUT_LEN\",\n",
      "            \"data_type\": \"TYPE_UINT32\",\n",
      "            \"dims\": [\n",
      "                -1\n",
      "            ],\n",
      "            \"label_filename\": \"\",\n",
      "            \"is_shape_tensor\": false\n",
      "        }\n",
      "    ],\n",
      "    \"batch_input\": [],\n",
      "    \"batch_output\": [],\n",
      "    \"optimization\": {\n",
      "        \"priority\": \"PRIORITY_DEFAULT\",\n",
      "        \"input_pinned_memory\": {\n",
      "            \"enable\": true\n",
      "        },\n",
      "        \"output_pinned_memory\": {\n",
      "            \"enable\": true\n",
      "        },\n",
      "        \"gather_kernel_buffer_threshold\": 0,\n",
      "        \"eager_batching\": false\n",
      "    },\n",
      "    \"instance_group\": [\n",
      "        {\n",
      "            \"name\": \"preprocessing_0\",\n",
      "            \"kind\": \"KIND_CPU\",\n",
      "            \"count\": 1,\n",
      "            \"gpus\": [],\n",
      "            \"secondary_devices\": [],\n",
      "            \"profile\": [],\n",
      "            \"passive\": false,\n",
      "            \"host_policy\": \"\"\n",
      "        }\n",
      "    ],\n",
      "    \"default_model_filename\": \"model.py\",\n",
      "    \"cc_model_filenames\": {},\n",
      "    \"metric_tags\": {},\n",
      "    \"parameters\": {\n",
      "        \"tokenizer_dir\": {\n",
      "            \"string_value\": \"/workspace/notebooks/tensorrt-llm/llama-models\"\n",
      "        },\n",
      "        \"tokenizer_type\": {\n",
      "            \"string_value\": \"llama\"\n",
      "        }\n",
      "    },\n",
      "    \"model_warmup\": []\n",
      "}\n",
      "I0304 16:08:07.201351 6090 python_be.cc:2199] TRITONBACKEND_ModelInstanceInitialize: preprocessing_0_0 (CPU device 0)\n",
      "I0304 16:08:07.201382 6090 backend_model_instance.cc:69] Creating instance preprocessing_0_0 on CPU using artifact 'model.py'\n",
      "I0304 16:08:07.201832 6090 stub_launcher.cc:254] Starting Python backend stub:  exec /opt/tritonserver/backends/python/triton_python_backend_stub /workspace/notebooks/tensorrt-llm/triton_model_repo/llama_7b/python/preprocessing/1/model.py triton_python_backend_shm_region_6 1048576 1048576 6090 /opt/tritonserver/backends/python 336 preprocessing_0_0 DEFAULT\n",
      "I0304 16:08:09.048091 6090 model.py:59] Loading HuggingFace model: /workspace/notebooks/tensorrt-llm/hf-weights/7B-chat...\n",
      "I0304 16:08:09.306457 6090 python_be.cc:2220] TRITONBACKEND_ModelInstanceInitialize: instance initialization successful preprocessing_0_0 (device 0)\n",
      "I0304 16:08:09.306579 6090 backend_model_instance.cc:772] Starting backend thread for preprocessing_0_0 at nice 0 on device 0...\n",
      "I0304 16:08:09.306660 6090 backend_model.cc:617] Created model instance named 'preprocessing_0_0' with device id '0'\n",
      "I0304 16:08:09.306794 6090 model_lifecycle.cc:675] OnLoadComplete() 'preprocessing' version 1\n",
      "I0304 16:08:09.306813 6090 model_lifecycle.cc:713] OnLoadFinal() 'preprocessing' for all version(s)\n",
      "I0304 16:08:09.306821 6090 model_lifecycle.cc:818] successfully loaded 'preprocessing'\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s][TensorRT-LLM][INFO] TRTGptModel maxNumSequences: 16\n",
      "[TensorRT-LLM][INFO] TRTGptModel maxBatchSize: 8\n",
      "[TensorRT-LLM][INFO] TRTGptModel enableTrtOverlap: 1\n",
      "[TensorRT-LLM][INFO] Loaded engine size: 12853 MiB\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 13140, GPU 14439 (MiB)\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Init cuDNN: CPU +1, GPU +42, now: CPU 13141, GPU 14481 (MiB)\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +12852, now: CPU 0, GPU 12852 (MiB)\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 13160, GPU 15755 (MiB)\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Init cuDNN: CPU +0, GPU +40, now: CPU 13160, GPU 15795 (MiB)\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 12852 (MiB)\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 13161, GPU 15889 (MiB)\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] Init cuDNN: CPU +1, GPU +10, now: CPU 13162, GPU 15899 (MiB)\n",
      "[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 12852 (MiB)\n",
      "I0304 16:08:38.381965 6090 backend_model_instance.cc:772] Starting backend thread for tensorrt_llm_0_0 at nice 0 on device 0...\n",
      "I0304 16:08:38.382036 6090 backend_model.cc:617] Created model instance named 'tensorrt_llm_0_0' with device id '0'\n",
      "I0304 16:08:38.382090 6090 model_lifecycle.cc:675] OnLoadComplete() 'tensorrt_llm' version 1\n",
      "I0304 16:08:38.382097 6090 model_lifecycle.cc:713] OnLoadFinal() 'tensorrt_llm' for all version(s)\n",
      "I0304 16:08:38.382103 6090 model_lifecycle.cc:818] successfully loaded 'tensorrt_llm'\n",
      "I0304 16:08:38.382155 6090 model_lifecycle.cc:286] VersionStates() 'tensorrt_llm'\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:29<00:00, 14.92s/it]\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "I0304 16:09:00.525237 6090 python_be.cc:2220] TRITONBACKEND_ModelInstanceInitialize: instance initialization successful llama_huggingface_0_0 (device 0)\n",
      "I0304 16:09:00.525359 6090 backend_model_instance.cc:772] Starting backend thread for llama_huggingface_0_0 at nice 0 on device 0...\n",
      "I0304 16:09:00.525506 6090 backend_model.cc:617] Created model instance named 'llama_huggingface_0_0' with device id '0'\n",
      "I0304 16:09:00.525648 6090 model_lifecycle.cc:675] OnLoadComplete() 'llama_huggingface' version 1\n",
      "I0304 16:09:00.525688 6090 model_lifecycle.cc:713] OnLoadFinal() 'llama_huggingface' for all version(s)\n",
      "I0304 16:09:00.525696 6090 model_lifecycle.cc:818] successfully loaded 'llama_huggingface'\n",
      "I0304 16:09:00.525737 6090 model_lifecycle.cc:286] VersionStates() 'llama_huggingface'\n",
      "I0304 16:09:00.525793 6090 model_lifecycle.cc:286] VersionStates() 'postprocessing'\n",
      "I0304 16:09:00.525802 6090 model_lifecycle.cc:286] VersionStates() 'preprocessing'\n",
      "I0304 16:09:00.525845 6090 model_lifecycle.cc:328] GetModel() 'preprocessing' version -1\n",
      "I0304 16:09:00.525897 6090 model_lifecycle.cc:328] GetModel() 'tensorrt_llm' version -1\n",
      "I0304 16:09:00.525945 6090 model_lifecycle.cc:328] GetModel() 'postprocessing' version -1\n",
      "I0304 16:09:00.525982 6090 model_lifecycle.cc:328] GetModel() 'preprocessing' version -1\n",
      "I0304 16:09:00.526004 6090 model_lifecycle.cc:328] GetModel() 'tensorrt_llm' version -1\n",
      "I0304 16:09:00.526039 6090 model_lifecycle.cc:328] GetModel() 'postprocessing' version -1\n",
      "I0304 16:09:00.526061 6090 model_lifecycle.cc:328] GetModel() 'preprocessing' version -1\n",
      "I0304 16:09:00.526079 6090 model_lifecycle.cc:328] GetModel() 'tensorrt_llm' version -1\n",
      "I0304 16:09:00.526108 6090 model_lifecycle.cc:328] GetModel() 'postprocessing' version -1\n",
      "I0304 16:09:00.526127 6090 model_lifecycle.cc:430] AsyncLoad() 'ensemble'\n",
      "W0304 16:09:00.527048 6090 model_lifecycle.cc:108] ignore version directory '.ipynb_checkpoints' which fails to convert to integral number\n",
      "I0304 16:09:00.527075 6090 model_lifecycle.cc:461] loading: ensemble:1\n",
      "I0304 16:09:00.527103 6090 model_lifecycle.cc:531] CreateModel() 'ensemble' version 1\n",
      "I0304 16:09:00.527268 6090 ensemble_model.cc:55] ensemble model for ensemble\n",
      "\n",
      "I0304 16:09:00.527278 6090 model_lifecycle.cc:328] GetModel() 'postprocessing' version -1\n",
      "I0304 16:09:00.527287 6090 model_lifecycle.cc:675] OnLoadComplete() 'ensemble' version 1\n",
      "I0304 16:09:00.527292 6090 model_lifecycle.cc:713] OnLoadFinal() 'ensemble' for all version(s)\n",
      "I0304 16:09:00.527298 6090 model_lifecycle.cc:818] successfully loaded 'ensemble'\n",
      "I0304 16:09:00.527310 6090 model_lifecycle.cc:286] VersionStates() 'ensemble'\n",
      "I0304 16:09:00.527322 6090 model_lifecycle.cc:286] VersionStates() 'tensorrt_llm'\n",
      "I0304 16:09:00.527326 6090 model_lifecycle.cc:286] VersionStates() 'postprocessing'\n",
      "I0304 16:09:00.527328 6090 model_lifecycle.cc:286] VersionStates() 'llama_huggingface'\n",
      "I0304 16:09:00.527332 6090 model_lifecycle.cc:286] VersionStates() 'preprocessing'\n",
      "I0304 16:09:00.527335 6090 model_lifecycle.cc:286] VersionStates() 'ensemble'\n",
      "I0304 16:09:00.527367 6090 server.cc:592] \n",
      "+------------------+------+\n",
      "| Repository Agent | Path |\n",
      "+------------------+------+\n",
      "+------------------+------+\n",
      "\n",
      "I0304 16:09:00.527397 6090 server.cc:619] \n",
      "+-------------+-------------------------------+-------------------------------+\n",
      "| Backend     | Path                          | Config                        |\n",
      "+-------------+-------------------------------+-------------------------------+\n",
      "| tensorrtllm | /opt/tritonserver/backends/te | {\"cmdline\":{\"auto-complete-co |\n",
      "|             | nsorrtllm/libtriton_tensorrtl | nfig\":\"true\",\"backend-directo |\n",
      "|             | lm.so                         | ry\":\"/opt/tritonserver/backen |\n",
      "|             |                               | ds\",\"min-compute-capability\": |\n",
      "|             |                               | \"6.000000\",\"default-max-batch |\n",
      "|             |                               | -size\":\"4\"}}                  |\n",
      "|             |                               |                               |\n",
      "|             |                               |                               |\n",
      "| python      | /opt/tritonserver/backends/py | {\"cmdline\":{\"auto-complete-co |\n",
      "|             | thon/libtriton_python.so      | nfig\":\"true\",\"backend-directo |\n",
      "|             |                               | ry\":\"/opt/tritonserver/backen |\n",
      "|             |                               | ds\",\"min-compute-capability\": |\n",
      "|             |                               | \"6.000000\",\"default-max-batch |\n",
      "|             |                               | -size\":\"4\"}}                  |\n",
      "|             |                               |                               |\n",
      "+-------------+-------------------------------+-------------------------------+\n",
      "\n",
      "I0304 16:09:00.527415 6090 model_lifecycle.cc:265] ModelStates()\n",
      "I0304 16:09:00.527438 6090 server.cc:662] \n",
      "+-------------------+---------+--------+\n",
      "| Model             | Version | Status |\n",
      "+-------------------+---------+--------+\n",
      "| ensemble          | 1       | READY  |\n",
      "| llama_huggingface | 1       | READY  |\n",
      "| postprocessing    | 1       | READY  |\n",
      "| preprocessing     | 1       | READY  |\n",
      "| tensorrt_llm      | 1       | READY  |\n",
      "+-------------------+---------+--------+\n",
      "\n",
      "I0304 16:09:00.563493 6090 metrics.cc:817] Collecting metrics for GPU 0: NVIDIA A100-SXM4-80GB\n",
      "I0304 16:09:00.564505 6090 metrics.cc:710] Collecting CPU metrics\n",
      "I0304 16:09:00.564673 6090 tritonserver.cc:2458] \n",
      "+----------------------------------+------------------------------------------+\n",
      "| Option                           | Value                                    |\n",
      "+----------------------------------+------------------------------------------+\n",
      "| server_id                        | triton                                   |\n",
      "| server_version                   | 2.39.0                                   |\n",
      "| server_extensions                | classification sequence model_repository |\n",
      "|                                  |  model_repository(unload_dependents) sch |\n",
      "|                                  | edule_policy model_configuration system_ |\n",
      "|                                  | shared_memory cuda_shared_memory binary_ |\n",
      "|                                  | tensor_data parameters statistics trace  |\n",
      "|                                  | logging                                  |\n",
      "| model_repository_path[0]         | /workspace/notebooks/tensorrt-llm/triton |\n",
      "|                                  | _model_repo/llama_7b/python              |\n",
      "| model_control_mode               | MODE_NONE                                |\n",
      "| strict_model_config              | 0                                        |\n",
      "| rate_limit                       | OFF                                      |\n",
      "| pinned_memory_pool_byte_size     | 268435456                                |\n",
      "| cuda_memory_pool_byte_size{0}    | 67108864                                 |\n",
      "| min_supported_compute_capability | 6.0                                      |\n",
      "| strict_readiness                 | 1                                        |\n",
      "| exit_timeout                     | 30                                       |\n",
      "| cache_enabled                    | 0                                        |\n",
      "+----------------------------------+------------------------------------------+\n",
      "\n",
      "I0304 16:09:00.565835 6090 grpc_server.cc:2407] \n",
      "+----------------------------------------------+---------+\n",
      "| GRPC KeepAlive Option                        | Value   |\n",
      "+----------------------------------------------+---------+\n",
      "| keepalive_time_ms                            | 7200000 |\n",
      "| keepalive_timeout_ms                         | 20000   |\n",
      "| keepalive_permit_without_calls               | 0       |\n",
      "| http2_max_pings_without_data                 | 2       |\n",
      "| http2_min_recv_ping_interval_without_data_ms | 300000  |\n",
      "| http2_max_ping_strikes                       | 2       |\n",
      "+----------------------------------------------+---------+\n",
      "\n",
      "I0304 16:09:00.566307 6090 grpc_server.cc:101] Ready for RPC 'Check', 0\n",
      "I0304 16:09:00.566330 6090 grpc_server.cc:101] Ready for RPC 'ServerLive', 0\n",
      "I0304 16:09:00.566340 6090 grpc_server.cc:101] Ready for RPC 'ServerReady', 0\n",
      "I0304 16:09:00.566345 6090 grpc_server.cc:101] Ready for RPC 'ModelReady', 0\n",
      "I0304 16:09:00.566350 6090 grpc_server.cc:101] Ready for RPC 'ServerMetadata', 0\n",
      "I0304 16:09:00.566356 6090 grpc_server.cc:101] Ready for RPC 'ModelMetadata', 0\n",
      "I0304 16:09:00.566362 6090 grpc_server.cc:101] Ready for RPC 'ModelConfig', 0\n",
      "I0304 16:09:00.566372 6090 grpc_server.cc:101] Ready for RPC 'SystemSharedMemoryStatus', 0\n",
      "I0304 16:09:00.566378 6090 grpc_server.cc:101] Ready for RPC 'SystemSharedMemoryRegister', 0\n",
      "I0304 16:09:00.566383 6090 grpc_server.cc:101] Ready for RPC 'SystemSharedMemoryUnregister', 0\n",
      "I0304 16:09:00.566390 6090 grpc_server.cc:101] Ready for RPC 'CudaSharedMemoryStatus', 0\n",
      "I0304 16:09:00.566394 6090 grpc_server.cc:101] Ready for RPC 'CudaSharedMemoryRegister', 0\n",
      "I0304 16:09:00.566399 6090 grpc_server.cc:101] Ready for RPC 'CudaSharedMemoryUnregister', 0\n",
      "I0304 16:09:00.566405 6090 grpc_server.cc:101] Ready for RPC 'RepositoryIndex', 0\n",
      "I0304 16:09:00.566411 6090 grpc_server.cc:101] Ready for RPC 'RepositoryModelLoad', 0\n",
      "I0304 16:09:00.566416 6090 grpc_server.cc:101] Ready for RPC 'RepositoryModelUnload', 0\n",
      "I0304 16:09:00.566423 6090 grpc_server.cc:101] Ready for RPC 'ModelStatistics', 0\n",
      "I0304 16:09:00.566428 6090 grpc_server.cc:101] Ready for RPC 'Trace', 0\n",
      "I0304 16:09:00.566434 6090 grpc_server.cc:101] Ready for RPC 'Logging', 0\n",
      "I0304 16:09:00.566446 6090 grpc_server.cc:350] Thread started for CommonHandler\n",
      "I0304 16:09:00.566542 6090 infer_handler.h:1176] StateNew, 0 Step START\n",
      "I0304 16:09:00.566554 6090 infer_handler.cc:673] New request handler for ModelInferHandler, 0\n",
      "I0304 16:09:00.566562 6090 infer_handler.h:1300] Thread started for ModelInferHandler\n",
      "I0304 16:09:00.566628 6090 infer_handler.h:1176] StateNew, 0 Step START\n",
      "I0304 16:09:00.566640 6090 infer_handler.cc:673] New request handler for ModelInferHandler, 0\n",
      "I0304 16:09:00.566649 6090 infer_handler.h:1300] Thread started for ModelInferHandler\n",
      "I0304 16:09:00.566711 6090 infer_handler.h:1176] StateNew, 0 Step START\n",
      "I0304 16:09:00.566717 6090 stream_infer_handler.cc:128] New request handler for ModelStreamInferHandler, 0\n",
      "I0304 16:09:00.566724 6090 infer_handler.h:1300] Thread started for ModelStreamInferHandler\n",
      "I0304 16:09:00.566728 6090 grpc_server.cc:2513] Started GRPCInferenceService at 0.0.0.0:8001\n",
      "I0304 16:09:00.566903 6090 http_server.cc:4497] Started HTTPService at 0.0.0.0:8000\n",
      "I0304 16:09:00.608683 6090 http_server.cc:270] Started Metrics Service at 0.0.0.0:8002\n",
      "^C\n",
      "Signal (2) received.\n",
      "I0304 16:09:55.997777 6090 server.cc:293] Waiting for in-flight requests to complete.\n",
      "I0304 16:09:55.997804 6090 model_lifecycle.cc:223] StopAllModels()\n",
      "I0304 16:09:55.997819 6090 model_lifecycle.cc:241] InflightStatus()\n",
      "I0304 16:09:55.997826 6090 server.cc:309] Timeout 30: Found 0 model versions that have in-flight inferences\n",
      "I0304 16:09:55.997832 6090 model_lifecycle.cc:382] AsyncUnload() 'tensorrt_llm'\n",
      "I0304 16:09:55.997992 6090 model_lifecycle.cc:382] AsyncUnload() 'postprocessing'\n",
      "I0304 16:09:55.998133 6090 model_lifecycle.cc:382] AsyncUnload() 'llama_huggingface'\n",
      "I0304 16:09:55.998158 6090 backend_model_instance.cc:795] Stopping backend thread for tensorrt_llm_0_0...\n",
      "I0304 16:09:55.998228 6090 model_lifecycle.cc:382] AsyncUnload() 'preprocessing'\n",
      "I0304 16:09:55.998289 6090 backend_model_instance.cc:795] Stopping backend thread for postprocessing_0_0...\n",
      "I0304 16:09:55.998307 6090 backend_model_instance.cc:795] Stopping backend thread for llama_huggingface_0_0...\n",
      "I0304 16:09:55.998371 6090 model_lifecycle.cc:382] AsyncUnload() 'ensemble'\n",
      "I0304 16:09:55.998445 6090 backend_model_instance.cc:795] Stopping backend thread for preprocessing_0_0...\n",
      "I0304 16:09:55.998452 6090 python_be.cc:2339] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I0304 16:09:55.998530 6090 python_be.cc:2339] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I0304 16:09:55.998570 6090 server.cc:324] All models are stopped, unloading models\n",
      "I0304 16:09:55.998579 6090 model_lifecycle.cc:190] LiveModelStates()\n",
      "I0304 16:09:55.998581 6090 python_be.cc:2339] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I0304 16:09:55.998591 6090 server.cc:331] Timeout 30: Found 5 live models and 0 in-flight non-inference requests\n",
      "I0304 16:09:55.998609 6090 server.cc:338] ensemble v1: UNLOADING\n",
      "I0304 16:09:55.998618 6090 server.cc:338] llama_huggingface v1: UNLOADING\n",
      "I0304 16:09:55.998624 6090 server.cc:338] postprocessing v1: UNLOADING\n",
      "I0304 16:09:55.998627 6090 server.cc:338] preprocessing v1: UNLOADING\n",
      "I0304 16:09:55.998630 6090 server.cc:338] tensorrt_llm v1: UNLOADING\n",
      "I0304 16:09:55.998714 6090 model_lifecycle.cc:601] OnDestroy callback() 'ensemble' version 1\n",
      "I0304 16:09:55.998732 6090 model_lifecycle.cc:603] successfully unloaded 'ensemble' version 1\n",
      "I0304 16:09:56.031250 6090 model_lifecycle.cc:601] OnDestroy callback() 'tensorrt_llm' version 1\n",
      "I0304 16:09:56.031269 6090 model_lifecycle.cc:603] successfully unloaded 'tensorrt_llm' version 1\n"
     ]
    }
   ],
   "source": [
    "## LAUNCH THIS COMMAND IN A SEPARATE TERMINAL - this server command needs to be kept alive\n",
    "\n",
    "!tritonserver --model-repository=/workspace/notebooks/tensorrt-llm/triton_model_repo/llama_7b/python # --log-verbose 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f7d363",
   "metadata": {},
   "source": [
    "You can verify that the triton server has successfully launch when you see terminal output such as below:\n",
    "```\n",
    "\n",
    "I0304 16:11:54.555571 7346 server.cc:662] \n",
    "+-------------------+---------+--------+\n",
    "| Model             | Version | Status |\n",
    "+-------------------+---------+--------+\n",
    "| ensemble          | 1       | READY  |\n",
    "| llama_huggingface | 1       | READY  |\n",
    "| postprocessing    | 1       | READY  |\n",
    "| preprocessing     | 1       | READY  |\n",
    "| tensorrt_llm      | 1       | READY  |\n",
    "+-------------------+---------+--------+\n",
    "\n",
    "I0304 16:11:54.593570 7346 metrics.cc:817] Collecting metrics for GPU 0: NVIDIA A100-SXM4-80GB\n",
    "I0304 16:11:54.594500 7346 metrics.cc:710] Collecting CPU metrics\n",
    "I0304 16:11:54.594653 7346 tritonserver.cc:2458] \n",
    "+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
    "| Option                           | Value                                                                                                                                                                                                           |\n",
    "+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
    "| server_id                        | triton                                                                                                                                                                                                          |\n",
    "| server_version                   | 2.39.0                                                                                                                                                                                                          |\n",
    "| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data parameters statistics trace logging |\n",
    "| model_repository_path[0]         | /workspace/notebooks/tensorrt-llm/triton_model_repo/llama_7b/python                                                                                                                                             |\n",
    "| model_control_mode               | MODE_NONE                                                                                                                                                                                                       |\n",
    "| strict_model_config              | 0                                                                                                                                                                                                               |\n",
    "| rate_limit                       | OFF                                                                                                                                                                                                             |\n",
    "| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                                       |\n",
    "| cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                                                                                                        |\n",
    "| min_supported_compute_capability | 6.0                                                                                                                                                                                                             |\n",
    "| strict_readiness                 | 1                                                                                                                                                                                                               |\n",
    "| exit_timeout                     | 30                                                                                                                                                                                                              |\n",
    "| cache_enabled                    | 0                                                                                                                                                                                                               |\n",
    "+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
    "\n",
    "I0304 16:11:54.596304 7346 grpc_server.cc:2513] Started GRPCInferenceService at 0.0.0.0:8001\n",
    "I0304 16:11:54.596487 7346 http_server.cc:4497] Started HTTPService at 0.0.0.0:8000\n",
    "I0304 16:11:54.637582 7346 http_server.cc:270] Started Metrics Service at 0.0.0.0:8002\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364d0d58",
   "metadata": {},
   "source": [
    "Now let's send inference requests to the triton server via triton client. To send an inflight inference request to Triton server, launch the following command using the provided client side script.\n",
    "\n",
    "Here we are sending request to the Python pipeline `llama_huggingface`, feel free to change `--model_name` to `ensemble` to send request to tensorRT-LLM pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d80597d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'I am going to be a little more specific about the types of things I would like to see in the future.\\n1. More detailed information about the different types of weapons and armor. For example, what are the strengths and weaknesses of each type of weapon? How does the armor work? What are the different types of armor and how do they protect the player?\\n2. More variety in the enemies. While the current enemies are interesting, I would like to see more variety in'\n"
     ]
    }
   ],
   "source": [
    "!python ./end_to_end_streaming_client.py -u localhost:8001 --model_name llama_huggingface --max_tokens 100  --prompt \"I am going to\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1167baa7",
   "metadata": {},
   "source": [
    "## 3. Benchmark Python pipeline vs TensorRT-LLM pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95ae01c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Now we are ready to benchmark the performance of TensorRT-LLM for llama-2-7b-chat inference vs the Python pipeline. A benchmark script `identity_test_python_vs_trtllm.py` is provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5031c250",
   "metadata": {},
   "source": [
    "Run the following command to benchmark the throughput of the huggingface Python pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ac0fb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Warm up for benchmarking.\n",
      "[INFO] Start benchmarking on 4 prompts.\n",
      "[INFO] Total Latency: 3948.6 ms\n"
     ]
    }
   ],
   "source": [
    "!python ./identity_test_python_vs_trtllm.py \\\n",
    "    -u localhost:8001 \\\n",
    "    --max_input_len 100 \\\n",
    "    --dataset /workspace/notebooks/tensorrt-llm/datasets/mini_cnn_eval.json \\\n",
    "    -i grpc \\\n",
    "    --model_name \"llama_huggingface\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c415ef",
   "metadata": {},
   "source": [
    "Run the following command to benchmark the throughput of the tensorRT-LLM pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4a28068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Warm up for benchmarking.\n",
      "[INFO] Start benchmarking on 4 prompts.\n",
      "[INFO] Total Latency: 724.201 ms\n"
     ]
    }
   ],
   "source": [
    "!python ./identity_test_python_vs_trtllm.py \\\n",
    "    -u localhost:8001 \\\n",
    "    --max_input_len 100 \\\n",
    "    --dataset /workspace/notebooks/tensorrt-llm/datasets/mini_cnn_eval.json \\\n",
    "    -i grpc \\\n",
    "    --model_name \"ensemble\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10670dcf",
   "metadata": {},
   "source": [
    "Depending on the type of GPU you are using, you can observe a different factor of speed-up for the latency measurement, typically around 4 - 5x."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b9a377",
   "metadata": {},
   "source": [
    "## 4. Going further\n",
    "\n",
    "We are not finished with TensorRT optimization yet, we can further push the optimization much further with techniques such as:\n",
    "- Parallelisation: pipeline and tensor\n",
    "- Inflight dynamic batching\n",
    "- Model quantization\n",
    "\n",
    "We will not cover these in this tutorial, but feel free to explore & test these optimizations by referring to the original demo [here](https://github.com/scaleway/ai-pulse-nvidia-trt-llm/tree/main/docs)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
